{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwcGr/cPtxV+q9Pg2ELtqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/split_main_data_to_training_and_validation_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKRZ66lixIjb",
        "outputId": "5fda9529-0e8e-40e7-efc0-a0d94f72fca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "print(\"Step 1: Reading CSV file...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-130.csv')\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Step 2 & 3: Calculate the ratio of each class (0, 1, 2)\n",
        "print(\"\\nStep 3: Calculating class ratios...\")\n",
        "class_counts = df['inflation'].value_counts().sort_index()\n",
        "total_records = len(df)\n",
        "\n",
        "print(f\"Class distribution:\")\n",
        "for class_label in [0, 1, 2]:\n",
        "    count = class_counts.get(class_label, 0)\n",
        "    ratio = count / total_records\n",
        "    print(f\"  Class {class_label}: {count} records ({ratio:.4f})\")\n",
        "\n",
        "# Step 4: Retrieve exactly half records while approximating class ratios\n",
        "print(\"\\nStep 4: Sampling exactly half records with approximate stratification...\")\n",
        "target_size = -(-total_records // 2)  # Ceiling division to round up half\n",
        "\n",
        "# Calculate proportional target counts for each class\n",
        "target_counts = {}\n",
        "remaining_target = target_size\n",
        "\n",
        "# First, calculate ideal proportional counts\n",
        "for class_label in [0, 1, 2]:\n",
        "    original_count = class_counts.get(class_label, 0)\n",
        "    original_ratio = original_count / total_records\n",
        "    ideal_count = int(target_size * original_ratio)\n",
        "    target_counts[class_label] = ideal_count\n",
        "\n",
        "# Adjust to ensure we get exactly target_size records\n",
        "current_total = sum(target_counts.values())\n",
        "difference = target_size - current_total\n",
        "\n",
        "# Distribute the difference to maintain ratios as closely as possible\n",
        "if difference != 0:\n",
        "    # Sort classes by their original size (largest first) to distribute difference\n",
        "    sorted_classes = sorted([0, 1, 2], key=lambda x: class_counts.get(x, 0), reverse=True)\n",
        "\n",
        "    # Add/subtract records starting from the largest class\n",
        "    for i, class_label in enumerate(sorted_classes):\n",
        "        if difference > 0:\n",
        "            if i < difference:\n",
        "                target_counts[class_label] += 1\n",
        "        else:  # difference < 0\n",
        "            if i < abs(difference):\n",
        "                target_counts[class_label] = max(0, target_counts[class_label] - 1)\n",
        "\n",
        "print(f\"Target sample size: {target_size}\")\n",
        "print(f\"Target counts per class:\")\n",
        "for class_label in [0, 1, 2]:\n",
        "    print(f\"  Class {class_label}: {target_counts[class_label]} records\")\n",
        "\n",
        "# Verify total\n",
        "actual_target_total = sum(target_counts.values())\n",
        "print(f\"Total target records: {actual_target_total}\")\n",
        "\n",
        "# Perform sampling\n",
        "sampled_dfs = []\n",
        "for class_label in [0, 1, 2]:\n",
        "    class_df = df[df['inflation'] == class_label]\n",
        "    target_count = target_counts[class_label]\n",
        "    if len(class_df) > 0 and target_count > 0:\n",
        "        sampled_class_df = class_df.sample(n=min(target_count, len(class_df)),\n",
        "                                         random_state=42)\n",
        "        sampled_dfs.append(sampled_class_df)\n",
        "\n",
        "# Combine sampled data\n",
        "sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "\n",
        "# Shuffle the combined data\n",
        "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSampled dataset shape: {sampled_df.shape}\")\n",
        "sampled_class_counts = sampled_df['inflation'].value_counts().sort_index()\n",
        "print(f\"Sampled class distribution:\")\n",
        "for class_label in [0, 1, 2]:\n",
        "    count = sampled_class_counts.get(class_label, 0)\n",
        "    ratio = count / len(sampled_df) if len(sampled_df) > 0 else 0\n",
        "    print(f\"  Class {class_label}: {count} records ({ratio:.4f})\")\n",
        "\n",
        "# Step 5: Save the retrieved records\n",
        "print(\"\\nStep 5: Saving sampled dataset...\")\n",
        "sampled_df.to_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-65.csv',\n",
        "                  index=False)\n",
        "print(\"Saved sampled dataset to main-prod-622.csv\")\n",
        "\n",
        "# Step 6: Split into training and validation data with stratification\n",
        "print(\"\\nStep 6: Splitting into training and validation data...\")\n",
        "X = sampled_df[['body']]\n",
        "y = sampled_df['inflation']\n",
        "\n",
        "# Use stratified split to maintain class ratios\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Combine features and labels back into DataFrames\n",
        "train_df = pd.concat([X_train, y_train], axis=1)\n",
        "val_df = pd.concat([X_val, y_val], axis=1)\n",
        "\n",
        "print(f\"Training set shape: {train_df.shape}\")\n",
        "print(f\"Validation set shape: {val_df.shape}\")\n",
        "\n",
        "# Verify class distributions in splits\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "train_class_counts = train_df['inflation'].value_counts().sort_index()\n",
        "for class_label in [0, 1, 2]:\n",
        "    count = train_class_counts.get(class_label, 0)\n",
        "    ratio = count / len(train_df) if len(train_df) > 0 else 0\n",
        "    print(f\"  Class {class_label}: {count} records ({ratio:.4f})\")\n",
        "\n",
        "print(f\"\\nValidation set class distribution:\")\n",
        "val_class_counts = val_df['inflation'].value_counts().sort_index()\n",
        "for class_label in [0, 1, 2]:\n",
        "    count = val_class_counts.get(class_label, 0)\n",
        "    ratio = count / len(val_df) if len(val_df) > 0 else 0\n",
        "    print(f\"  Class {class_label}: {count} records ({ratio:.4f})\")\n",
        "\n",
        "# Step 7: Save training and validation data\n",
        "print(\"\\nStep 7: Saving training and validation datasets...\")\n",
        "train_df.to_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/training-data-65.csv',\n",
        "                index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/validation-data-65.csv',\n",
        "              index=False)\n",
        "\n",
        "print(\"Saved training data to training-data-622.csv\")\n",
        "print(\"Saved validation data to validation-data-622.csv\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original dataset: {df.shape[0]} records\")\n",
        "print(f\"Sampled dataset: {sampled_df.shape[0]} records ({sampled_df.shape[0]/df.shape[0]:.1%})\")\n",
        "print(f\"Training set: {train_df.shape[0]} records ({train_df.shape[0]/sampled_df.shape[0]:.1%})\")\n",
        "print(f\"Validation set: {val_df.shape[0]} records ({val_df.shape[0]/sampled_df.shape[0]:.1%})\")\n",
        "\n",
        "# Verify that class ratios are preserved\n",
        "print(f\"\\nClass ratio preservation check:\")\n",
        "original_ratios = df['inflation'].value_counts(normalize=True).sort_index()\n",
        "sampled_ratios = sampled_df['inflation'].value_counts(normalize=True).sort_index()\n",
        "train_ratios = train_df['inflation'].value_counts(normalize=True).sort_index()\n",
        "val_ratios = val_df['inflation'].value_counts(normalize=True).sort_index()\n",
        "\n",
        "for class_label in [0, 1, 2]:\n",
        "    orig_ratio = original_ratios.get(class_label, 0)\n",
        "    samp_ratio = sampled_ratios.get(class_label, 0)\n",
        "    train_ratio = train_ratios.get(class_label, 0)\n",
        "    val_ratio = val_ratios.get(class_label, 0)\n",
        "\n",
        "    print(f\"Class {class_label}:\")\n",
        "    print(f\"  Original: {orig_ratio:.4f}\")\n",
        "    print(f\"  Sampled:  {samp_ratio:.4f} (diff: {abs(orig_ratio-samp_ratio):.4f})\")\n",
        "    print(f\"  Training: {train_ratio:.4f} (diff: {abs(orig_ratio-train_ratio):.4f})\")\n",
        "    print(f\"  Validation: {val_ratio:.4f} (diff: {abs(orig_ratio-val_ratio):.4f})\")\n",
        "\n",
        "print(\"\\nProcess completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsxqSp_DxO_h",
        "outputId": "9386e8ab-f25f-4e6a-8c28-336095dff8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Reading CSV file...\n",
            "Original dataset shape: (130, 2)\n",
            "Columns: ['body', 'inflation']\n",
            "\n",
            "Step 3: Calculating class ratios...\n",
            "Class distribution:\n",
            "  Class 0: 40 records (0.3077)\n",
            "  Class 1: 51 records (0.3923)\n",
            "  Class 2: 39 records (0.3000)\n",
            "\n",
            "Step 4: Sampling exactly half records with approximate stratification...\n",
            "Target sample size: 65\n",
            "Target counts per class:\n",
            "  Class 0: 20 records\n",
            "  Class 1: 26 records\n",
            "  Class 2: 19 records\n",
            "Total target records: 65\n",
            "\n",
            "Sampled dataset shape: (65, 2)\n",
            "Sampled class distribution:\n",
            "  Class 0: 20 records (0.3077)\n",
            "  Class 1: 26 records (0.4000)\n",
            "  Class 2: 19 records (0.2923)\n",
            "\n",
            "Step 5: Saving sampled dataset...\n",
            "Saved sampled dataset to main-prod-622.csv\n",
            "\n",
            "Step 6: Splitting into training and validation data...\n",
            "Training set shape: (48, 2)\n",
            "Validation set shape: (17, 2)\n",
            "\n",
            "Training set class distribution:\n",
            "  Class 0: 15 records (0.3125)\n",
            "  Class 1: 19 records (0.3958)\n",
            "  Class 2: 14 records (0.2917)\n",
            "\n",
            "Validation set class distribution:\n",
            "  Class 0: 5 records (0.2941)\n",
            "  Class 1: 7 records (0.4118)\n",
            "  Class 2: 5 records (0.2941)\n",
            "\n",
            "Step 7: Saving training and validation datasets...\n",
            "Saved training data to training-data-622.csv\n",
            "Saved validation data to validation-data-622.csv\n",
            "\n",
            "==================================================\n",
            "SUMMARY\n",
            "==================================================\n",
            "Original dataset: 130 records\n",
            "Sampled dataset: 65 records (50.0%)\n",
            "Training set: 48 records (73.8%)\n",
            "Validation set: 17 records (26.2%)\n",
            "\n",
            "Class ratio preservation check:\n",
            "Class 0:\n",
            "  Original: 0.3077\n",
            "  Sampled:  0.3077 (diff: 0.0000)\n",
            "  Training: 0.3125 (diff: 0.0048)\n",
            "  Validation: 0.2941 (diff: 0.0136)\n",
            "Class 1:\n",
            "  Original: 0.3923\n",
            "  Sampled:  0.4000 (diff: 0.0077)\n",
            "  Training: 0.3958 (diff: 0.0035)\n",
            "  Validation: 0.4118 (diff: 0.0195)\n",
            "Class 2:\n",
            "  Original: 0.3000\n",
            "  Sampled:  0.2923 (diff: 0.0077)\n",
            "  Training: 0.2917 (diff: 0.0083)\n",
            "  Validation: 0.2941 (diff: 0.0059)\n",
            "\n",
            "Process completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOYjf6kixm96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}