{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNA41sdkIcJxEdJ0uy67l0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/phi_2_7_performances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uDmy0UI1o21"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 1. INSTALL REQUIRED PACKAGES\n",
        "# --------------------------------------------------------------------------\n",
        "# Install necessary libraries for model evaluation and data handling.\n",
        "!pip install transformers==4.44.0 datasets scikit-learn matplotlib torch torchvision torchaudio accelerate bitsandbytes -q"
      ],
      "metadata": {
        "id": "nb2A-Ulm1sbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning model"
      ],
      "metadata": {
        "id": "EEbq0I_yfTpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 2. IMPORTS & INITIAL SETUP\n",
        "# --------------------------------------------------------------------------\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    PhiForSequenceClassification # Import the specific class for Phi-2\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3Cko1yFv16OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 3. HELPER CLASSES AND FUNCTIONS\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for handling tokenized test data.\"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized inputs and convert to tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Retrieve the corresponding label and convert to a tensor\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    \"\"\"Reads a CSV file into a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        # Assumes header is on the first row, and names the columns\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype={'body': 'str', 'inflation': 'int'})\n",
        "        print(f\"Successfully loaded {len(data)} records from {file_path}\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "# The prompt must be IDENTICAL to the one used during fine-tuning\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post: {post}\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    \"\"\"Applies the standard prompt format to a text post.\"\"\"\n",
        "    return INFLATION_PROMPT.format(post=str(post))\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 4. MAIN EVALUATION SCRIPT\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "TEST_DATA_PATH = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "MODEL_PATH = '/content/drive/MyDrive/world-inflation/data/model/Phi-3.5-fine-tuning/checkpoint-192'\n",
        "\n",
        "# --- Load and Prepare Data ---\n",
        "test_data = read_csv_file(TEST_DATA_PATH)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Apply the same prompt formatting as used in training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # --- Initialize Tokenizer ---\n",
        "    print(f\"\\nInitializing tokenizer for Phi-2...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2', trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Set padding token for Phi-2\n",
        "\n",
        "    # --- Tokenize Test Data ---\n",
        "    print(\"Tokenizing test data...\")\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512, # Use the same max_length as in training\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    test_labels = test_data['inflation'].tolist()\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # --- Load Fine-Tuned Model ---\n",
        "    print(f\"Loading fine-tuned model from: {MODEL_PATH}\")\n",
        "    try:\n",
        "        model = PhiForSequenceClassification.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            torch_dtype=torch.bfloat16, # Use bfloat16 for L4 GPU compatibility\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except OSError:\n",
        "        print(f\"Error: Model not found at {MODEL_PATH}.\")\n",
        "        print(\"Please ensure the path is correct and points to a valid checkpoint folder.\")\n",
        "        model = None\n",
        "\n",
        "    if model:\n",
        "        # --- Run Evaluation ---\n",
        "        test_loader = DataLoader(test_dataset, batch_size=8) # Batch size for evaluation\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "\n",
        "        print(\"\\nStarting evaluation...\")\n",
        "        with torch.no_grad(): # Disable gradient calculations for inference\n",
        "            for i, batch in enumerate(test_loader):\n",
        "                inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "                labels = batch['labels'].to(model.device)\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "                predicted_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    print(f\"  Processed { (i + 1) * test_loader.batch_size } samples...\")\n",
        "\n",
        "        print(\"Evaluation finished.\")\n",
        "\n",
        "        # --- Display Results ---\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Generate and print the classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        # The labels are 0, 1, 2. The target names map them to readable strings.\n",
        "        class_names = ['Deflation (0)', 'Neutral (1)', 'Inflation (2)']\n",
        "        report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "        print(report)\n",
        "\n",
        "        # Generate and plot the confusion matrix\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predicted_labels)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nEvaluation stopped because the test data could not be loaded.\")"
      ],
      "metadata": {
        "id": "WX2sMNIw181Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot model"
      ],
      "metadata": {
        "id": "N90nt92DfYvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 2. IMPORTS & INITIAL SETUP\n",
        "# --------------------------------------------------------------------------\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "pyFybvur2W3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 3. HELPER FUNCTIONS\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    \"\"\"Reads a CSV file into a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        # Assumes header is on the first row, and names the columns\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype={'body': 'str', 'inflation': 'int'})\n",
        "        print(f\"Successfully loaded {len(data)} records from {file_path}\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "# The prompt must be IDENTICAL to the one used during fine-tuning\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post: {post}\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    \"\"\"Applies the standard prompt format to a text post.\"\"\"\n",
        "    return INFLATION_PROMPT.format(post=str(post))\n",
        "\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"\n",
        "    Extract the classification (0, 1, or 2) from the model's response.\n",
        "    Uses multiple strategies to handle various response formats.\n",
        "    \"\"\"\n",
        "    # Strategy 1: Look for the exact number at the end or after \"Classification:\"\n",
        "    patterns = [\n",
        "        r'Classification:\\s*([012])',  # \"Classification: 0\"\n",
        "        r'Classification:\\s*(\\d)',     # \"Classification: 0\" (any digit)\n",
        "        r'\\b([012])\\b(?!.*[012])',     # Last occurrence of 0, 1, or 2 in the text\n",
        "        r'answer is\\s*([012])',        # \"answer is 0\"\n",
        "        r'category\\s*([012])',         # \"category 0\"\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            classification = int(match.group(1))\n",
        "            if classification in [0, 1, 2]:\n",
        "                return classification\n",
        "\n",
        "    # Strategy 2: Count occurrences of each class and return the most frequent\n",
        "    counts = {0: len(re.findall(r'\\b0\\b', response_text)),\n",
        "              1: len(re.findall(r'\\b1\\b', response_text)),\n",
        "              2: len(re.findall(r'\\b2\\b', response_text))}\n",
        "\n",
        "    if max(counts.values()) > 0:\n",
        "        return max(counts, key=counts.get)\n",
        "\n",
        "    # Strategy 3: Look for keywords indicating the class\n",
        "    response_lower = response_text.lower()\n",
        "    if any(word in response_lower for word in ['deflation', 'cheap', 'affordable', 'declining']):\n",
        "        return 0\n",
        "    elif any(word in response_lower for word in ['inflation', 'expensive', 'overpriced', 'costly']):\n",
        "        return 2\n",
        "    else:\n",
        "        return 1  # Default to neutral if unclear\n",
        "\n",
        "def zero_shot_predict(model, tokenizer, formatted_posts, batch_size=4):\n",
        "    \"\"\"\n",
        "    Generate zero-shot predictions for a list of formatted posts.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    total_posts = len(formatted_posts)\n",
        "\n",
        "    print(f\"Generating predictions for {total_posts} posts...\")\n",
        "\n",
        "    for i in range(0, total_posts, batch_size):\n",
        "        batch_posts = formatted_posts[i:i+batch_size]\n",
        "        batch_predictions = []\n",
        "\n",
        "        for post in batch_posts:\n",
        "            try:\n",
        "                # Tokenize the input\n",
        "                inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Generate response\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=50,  # Limit response length\n",
        "                        temperature=0.1,    # Low temperature for more deterministic output\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        eos_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                # Decode the response\n",
        "                full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                # Extract only the generated part (after the prompt)\n",
        "                response = full_response[len(post):].strip()\n",
        "\n",
        "                # Extract classification from response\n",
        "                classification = extract_classification(response)\n",
        "                batch_predictions.append(classification)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing post: {e}\")\n",
        "                batch_predictions.append(1)  # Default to neutral on error\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "        # Progress update\n",
        "        processed = min(i + batch_size, total_posts)\n",
        "        if processed % 20 == 0 or processed == total_posts:\n",
        "            print(f\"  Processed {processed}/{total_posts} samples...\")\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "MW9EYAKNfYKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# 4. MAIN ZERO-SHOT EVALUATION SCRIPT\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "TEST_DATA_PATH = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# --- Load and Prepare Data ---\n",
        "test_data = read_csv_file(TEST_DATA_PATH)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Apply the same prompt formatting as used in training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # --- Initialize Tokenizer and Model ---\n",
        "    print(f\"\\nInitializing tokenizer and model for Phi-2...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2', trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load the base Phi-2 model for zero-shot inference\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            'microsoft/phi-2',\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model.eval()\n",
        "        print(\"Base Phi-2 model loaded successfully for zero-shot evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        model = None\n",
        "\n",
        "    if model:\n",
        "        # --- Run Zero-Shot Evaluation ---\n",
        "        true_labels = test_data['inflation'].tolist()\n",
        "        formatted_posts = test_data['formatted_body'].tolist()\n",
        "\n",
        "        print(\"\\nStarting zero-shot evaluation...\")\n",
        "        predicted_labels = zero_shot_predict(model, tokenizer, formatted_posts, batch_size=4)\n",
        "\n",
        "        print(\"Zero-shot evaluation finished.\")\n",
        "\n",
        "        # --- Display Results ---\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        print(f\"\\nZero-Shot Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Generate and print the classification report\n",
        "        print(\"\\nZero-Shot Classification Report:\")\n",
        "        class_names = ['Deflation (0)', 'Neutral (1)', 'Inflation (2)']\n",
        "        report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "        print(report)\n",
        "\n",
        "        # Generate and plot the confusion matrix\n",
        "        print(\"\\nZero-Shot Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predicted_labels)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Zero-Shot Confusion Matrix - Base Phi-2')\n",
        "        plt.show()\n",
        "\n",
        "        # --- Additional Analysis ---\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"COMPARISON SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"This is the zero-shot performance of the base Phi-2 model.\")\n",
        "        print(\"Compare these results with your fine-tuned model to measure\")\n",
        "        print(\"the improvement gained from fine-tuning.\")\n",
        "\n",
        "        # Show some example predictions for manual inspection\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SAMPLE PREDICTIONS (First 5)\")\n",
        "        print(\"=\"*50)\n",
        "        for i in range(min(5, len(true_labels))):\n",
        "            print(f\"\\nSample {i+1}:\")\n",
        "            print(f\"Post: {test_data['body'].iloc[i][:100]}...\")\n",
        "            print(f\"True Label: {true_labels[i]} ({class_names[true_labels[i]]})\")\n",
        "            print(f\"Predicted: {predicted_labels[i]} ({class_names[predicted_labels[i]]})\")\n",
        "            print(f\"Correct: {'✓' if true_labels[i] == predicted_labels[i] else '✗'}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nEvaluation stopped because the test data could not be loaded.\")"
      ],
      "metadata": {
        "id": "0a5tET5EflWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUqxT9Rxfqyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}