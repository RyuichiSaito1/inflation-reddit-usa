{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4Xj1KVAX637gCdUXY9FKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws5i3gpaUzKk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "-hpUFhadVe4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the TSV file\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_food_results_with_year_month.tsv'\n",
        "file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_food_results_20260819_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_cars_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_RealEstate_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_travel_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_Frugal_results_with_year_month.tsv'\n",
        "\n",
        "# Read the TSV file into a DataFrame\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "p3B10ksHU1JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# food\n",
        "# filtered_df = df[(df['year_month'] >= '2016-12') & (df['year_month'] <= '2020-05') & (df['inflation'] == 2)]\n",
        "filtered_df = df[(df['year_month'] >= '2020-06') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# cars\n",
        "# filtered_df = df[(df['year_month'] >= '2018-07') & (df['year_month'] <= '2020-11') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-11') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# Real Estate\n",
        "# filtered_df = df[(df['year_month'] >= '2015-05') & (df['year_month'] <= '2020-07') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-08') & (df['year_month'] <= '2022-07') & (df['inflation'] == 2)]\n",
        "# Travel\n",
        "# filtered_df = df[(df['year_month'] >= '2019-02') & (df['year_month'] <= '2020-06') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-07') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# Frugal\n",
        "# filtered_df = df[(df['year_month'] >= '2015-01') & (df['year_month'] <= '2020-10') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-11') & (df['year_month'] <= '2022-04') & (df['inflation'] == 2)]\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "tgBd4VCeVBxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English tokenizer from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Extract the 'body' column and apply stemming\n",
        "stemmed_texts = filtered_df['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
        "\n",
        "# Display the first 10 stemmed results\n",
        "for stemmed_text in stemmed_texts.head(10):\n",
        "    print(stemmed_text)\n"
      ],
      "metadata": {
        "id": "fFCZ0lNlVUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Clean the text by removing stopwords, numbers, and noise\n",
        "cleaned_texts = stemmed_texts.apply(\n",
        "    lambda text: ' '.join([token.text for token in nlp(text)\n",
        "                          if not token.is_stop and token.is_alpha and not re.search(r'\\d', token.text)])\n",
        ")\n",
        "\n",
        "# Convert cleaned_texts to a list of strings, as required by CountVectorizer\n",
        "cleaned_texts_list = cleaned_texts.tolist()\n",
        "\n",
        "# Display the first 10 cleaned results (stopword removed, numbers, and noise removed)\n",
        "for cleaned_text in cleaned_texts_list[:10]:  # Use cleaned_texts_list instead\n",
        "    print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "5IdlzqAKVlZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Step 1: Create Co-occurrence Matrix\n",
        "# Initialize the CountVectorizer to count word frequencies, excluding common English stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X = vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create the co-occurrence matrix by multiplying the term-document matrix with its transpose\n",
        "co_occurrence_matrix = (X.T * X).toarray()\n",
        "\n",
        "# Convert the co-occurrence matrix into a DataFrame for better readability\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=words, columns=words)\n",
        "print(\"Co-occurrence Matrix:\")\n",
        "print(co_occurrence_df)"
      ],
      "metadata": {
        "id": "1lN7CzLWVndm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 2: Create TF-IDF Matrix\n",
        "\n",
        "# # List of words to exclude\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# # exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# # exclude_words = ['real', 'real', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'york', 'san', 'los'\n",
        "# ]\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(2, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "id": "16Gc6oNzV2v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "food before COVID"
      ],
      "metadata": {
        "id": "Igi0M8KiPxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTX3xkBIPb9e",
        "outputId": "002cf1a5-1b28-4f83-bae5-9ea2ca71d7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          143.422522\n",
            "good          131.225539\n",
            "pricey         73.566501\n",
            "think          73.119475\n",
            "eat            71.785299\n",
            "place          70.814055\n",
            "pretty         67.808561\n",
            "buy            67.017478\n",
            "worth          66.710840\n",
            "use            66.628557\n",
            "meat           66.495964\n",
            "restaurant     65.199364\n",
            "try            64.921129\n",
            "time           64.413207\n",
            "live           63.462105\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look delicious    21.100400\n",
            "look good         19.692833\n",
            "lobster roll      14.718989\n",
            "bit pricey        14.612192\n",
            "grocery store     14.234269\n",
            "ice cream         12.884045\n",
            "year ago          12.101452\n",
            "shake shack       11.954780\n",
            "pretty good       11.923791\n",
            "little pricey     11.416855\n",
            "look amazing      11.294659\n",
            "want try          10.499790\n",
            "hard find         10.175283\n",
            "kobe beef          9.907465\n",
            "taste good         9.263085\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food after COVID"
      ],
      "metadata": {
        "id": "a-4EwU8YPu5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kihjuW21OfNZ",
        "outputId": "e04475f4-716f-449e-dbb0-3743fa5e05f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          79.397912\n",
            "good          66.175000\n",
            "use           43.273804\n",
            "meat          39.153459\n",
            "eat           36.914857\n",
            "think         36.711093\n",
            "pricey        36.236954\n",
            "time          36.221291\n",
            "place         33.670440\n",
            "restaurant    33.498952\n",
            "buy           33.345404\n",
            "way           33.013601\n",
            "lobster       32.959377\n",
            "worth         32.565316\n",
            "live          31.108129\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look good         12.940388\n",
            "lobster roll      10.114863\n",
            "look delicious     9.221660\n",
            "grocery store      8.973331\n",
            "year ago           8.609246\n",
            "look great         7.706228\n",
            "look amazing       7.515160\n",
            "bit pricey         6.681978\n",
            "hard find          5.399762\n",
            "hot dog            4.858838\n",
            "little pricey      4.800015\n",
            "high quality       4.790216\n",
            "new york           4.500146\n",
            "short rib          4.343338\n",
            "pretty good        4.102249\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cars before COVID"
      ],
      "metadata": {
        "id": "EXxvX3BJbH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxi9D7mlbFez",
        "outputId": "39fe698e-8c27-4d1f-d4d7-e5d250a449a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "car       196.102963\n",
            "buy        70.604764\n",
            "new        67.964061\n",
            "year       61.801178\n",
            "look       61.105670\n",
            "people     59.970622\n",
            "think      59.434780\n",
            "use        58.251591\n",
            "drive      57.197795\n",
            "want       54.505507\n",
            "high       53.545882\n",
            "good       50.425899\n",
            "time       47.036306\n",
            "lot        45.567389\n",
            "know       44.088570\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "new car       22.721342\n",
            "year old      15.208491\n",
            "sport car     12.700061\n",
            "buy new       12.249622\n",
            "use car       12.071384\n",
            "buy car       10.846983\n",
            "brand new     10.810440\n",
            "year ago       9.570674\n",
            "old car        9.305020\n",
            "people buy     9.021110\n",
            "car year       8.767848\n",
            "luxury car     8.006060\n",
            "base model     7.462739\n",
            "car look       6.827017\n",
            "car worth      6.735325\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cars after COVID"
      ],
      "metadata": {
        "id": "hHTufqsPak9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "id": "nojeKxPIWcXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d859ea-0e31-411d-cc37-badb74c5414d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "car        200.507860\n",
            "buy         76.074557\n",
            "new         72.703472\n",
            "people      69.125604\n",
            "year        68.889828\n",
            "use         60.048779\n",
            "drive       59.936499\n",
            "high        58.821536\n",
            "think       56.870891\n",
            "look        56.169809\n",
            "want        55.118786\n",
            "time        50.533413\n",
            "good        49.437796\n",
            "vehicle     49.309129\n",
            "pay         45.887642\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "new car         26.687530\n",
            "use car         19.210675\n",
            "year ago        15.003095\n",
            "sport car       14.992450\n",
            "buy car         13.183247\n",
            "buy new         12.940419\n",
            "year old        12.879925\n",
            "brand new       11.677048\n",
            "people buy      10.547999\n",
            "luxury car       8.501804\n",
            "car car          8.152328\n",
            "car year         8.057843\n",
            "old car          7.963277\n",
            "electric car     7.538813\n",
            "car market       7.506092\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RealEstate before COVID"
      ],
      "metadata": {
        "id": "Wv_2JSqQf1NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# cars\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL7sqarXfy1T",
        "outputId": "3453ac26-2d04-4dae-be4d-fd79d666980a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "house       242.176073\n",
            "home        206.740487\n",
            "market      155.139378\n",
            "buy         151.638467\n",
            "year        150.861808\n",
            "high        139.554748\n",
            "property    132.952090\n",
            "area        132.643980\n",
            "pay         129.690492\n",
            "offer       121.659441\n",
            "sell        120.687299\n",
            "look        120.330552\n",
            "rent        119.200908\n",
            "want        115.951669\n",
            "time        106.300133\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "buy house         44.545499\n",
            "interest rate     34.193197\n",
            "buy home          26.893986\n",
            "year ago          25.726018\n",
            "bay area          24.413026\n",
            "property taxis    21.122941\n",
            "sell house        20.770665\n",
            "hot market        17.785955\n",
            "house sell        17.588880\n",
            "good luck         17.283839\n",
            "single family     15.936882\n",
            "seller market     15.619880\n",
            "long term         15.505065\n",
            "sq ft             14.894357\n",
            "look house        14.758237\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RealEstate after COVID"
      ],
      "metadata": {
        "id": "G6xDAF7DeFKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# cars\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_KvKS8-aYf8",
        "outputId": "5ea7f592-1a3b-4c98-9094-4dd12491d53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "house     163.578109\n",
            "home      146.224377\n",
            "market    110.672748\n",
            "year      107.328061\n",
            "buy       106.374423\n",
            "people     90.402172\n",
            "high       89.368830\n",
            "rate       87.152895\n",
            "offer      86.206788\n",
            "sell       81.365060\n",
            "pay        75.598666\n",
            "area       71.994751\n",
            "want       71.636574\n",
            "rent       71.185480\n",
            "think      68.297565\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "interest rate       38.193469\n",
            "buy house           32.042560\n",
            "year ago            19.712706\n",
            "buy home            18.907245\n",
            "housing market      13.764943\n",
            "house sell          13.012088\n",
            "sell house          12.884586\n",
            "new construction    12.352712\n",
            "offer ask           11.990217\n",
            "property taxis      11.802241\n",
            "home sell           11.282815\n",
            "month ago           10.642864\n",
            "new home             9.996990\n",
            "bay area             9.866553\n",
            "long term            9.838507\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "travel before COVID"
      ],
      "metadata": {
        "id": "hZL7kdEJv0R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "exclude_words = [\n",
        "    'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "    'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "    'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "    'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "    'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "    'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "    'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "    'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "    'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "    'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "    'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "    'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "    'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "    'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "    'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "    'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "    'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "    'baton rouge', 'richmond', 'hialeah',\n",
        "    'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "    'amtrak', 'greyhound', 'interstate', 'san', 'york', 'los'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_TNyoWgvs6t",
        "outputId": "622f8633-c4e0-49f3-cceb-5c875fb3d644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['across', 'ana', 'angeles', 'antonio', 'baton', 'beach', 'canyon', 'carolina', 'central', 'christi', 'chula', 'city', 'clarita', 'corpus', 'dakota', 'dc', 'diego', 'disney', 'el', 'fort', 'francisco', 'grand', 'hampshire', 'island', 'jersey', 'jose', 'las', 'long', 'louis', 'mexico', 'new', 'north', 'orleans', 'park', 'paso', 'paul', 'petersburg', 'rhode', 'rouge', 'salem', 'santa', 'south', 'springs', 'st', 'states', 'the', 'united', 'us', 'vista', 'wayne', 'west', 'winston', 'world', 'worth'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "day       33.859492\n",
            "flight    32.539139\n",
            "city      29.374193\n",
            "time      29.035391\n",
            "trip      26.757787\n",
            "good      25.484383\n",
            "place     25.289780\n",
            "car       25.237215\n",
            "want      24.052174\n",
            "look      22.926905\n",
            "fly       22.281590\n",
            "way       22.263643\n",
            "think     21.734675\n",
            "drive     20.494979\n",
            "people    20.220639\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "rent car         9.630757\n",
            "national park    5.175079\n",
            "spend day        4.751070\n",
            "day trip         4.724252\n",
            "round trip       4.619062\n",
            "new orleans      4.430840\n",
            "grand canyon     4.270544\n",
            "car rental       4.199778\n",
            "east coast       3.825993\n",
            "road trip        3.750314\n",
            "spend time       3.617164\n",
            "west coast       3.572837\n",
            "time year        3.516019\n",
            "big city         3.189684\n",
            "good luck        3.130565\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "travel after COVID"
      ],
      "metadata": {
        "id": "7S3L0dxnvxlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "exclude_words = [\n",
        "    'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "    'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "    'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "    'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "    'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "    'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "    'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "    'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "    'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "    'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "    'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "    'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "    'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "    'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "    'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "    'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "    'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "    'baton rouge', 'richmond', 'hialeah',\n",
        "    'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "    'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "507j7dukeBzx",
        "outputId": "8e6f85ff-7e49-4718-d05b-555284ee1a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['across', 'ana', 'angeles', 'antonio', 'baton', 'beach', 'canyon', 'carolina', 'central', 'christi', 'chula', 'city', 'clarita', 'corpus', 'dakota', 'dc', 'diego', 'disney', 'el', 'fort', 'francisco', 'grand', 'hampshire', 'island', 'jersey', 'jose', 'las', 'long', 'los', 'louis', 'mexico', 'new', 'north', 'orleans', 'park', 'paso', 'paul', 'petersburg', 'rhode', 'rouge', 'salem', 'santa', 'south', 'springs', 'st', 'states', 'the', 'united', 'us', 'vista', 'wayne', 'west', 'winston', 'world', 'worth'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "time      57.451589\n",
            "day       56.442518\n",
            "flight    52.878730\n",
            "city      52.748550\n",
            "place     51.841348\n",
            "trip      51.107792\n",
            "car       50.037077\n",
            "want      47.499021\n",
            "good      46.597402\n",
            "think     41.017668\n",
            "lot       39.575301\n",
            "hotel     39.448018\n",
            "people    38.350848\n",
            "year      38.166532\n",
            "fly       37.897906\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "rent car                 16.328664\n",
            "rental car               12.400245\n",
            "car rental               11.360510\n",
            "national park            10.192015\n",
            "year ago                  9.594561\n",
            "new orleans               8.945887\n",
            "day trip                  8.753072\n",
            "costa rica                7.577532\n",
            "time year                 7.351130\n",
            "west coast                7.338721\n",
            "public transportation     6.764479\n",
            "round trip                6.754204\n",
            "road trip                 6.454885\n",
            "east coast                6.444626\n",
            "good luck                 6.202151\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frugal before COVID"
      ],
      "metadata": {
        "id": "KMP_JHfpMWUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jNgfQ46MQUH",
        "outputId": "9e81a8f8-0c63-41e2-9d00-f79a9923d522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "buy      147.903227\n",
            "use      123.888450\n",
            "car      114.176110\n",
            "year     112.221775\n",
            "good     109.400008\n",
            "pay      107.138028\n",
            "time     101.517401\n",
            "look      94.463426\n",
            "live      94.097667\n",
            "money     91.027264\n",
            "want      90.885079\n",
            "month     90.704992\n",
            "need      90.338385\n",
            "thing     89.535526\n",
            "find      89.297772\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "save money       28.098878\n",
            "grocery store    22.366959\n",
            "year ago         18.651457\n",
            "year old         15.268253\n",
            "buy new          14.539080\n",
            "good deal        13.025636\n",
            "new car          11.538842\n",
            "spend money      11.107503\n",
            "pay month        10.535364\n",
            "long time        10.522145\n",
            "high quality     10.506620\n",
            "lot money        10.108226\n",
            "credit card       9.565503\n",
            "thrift store      9.353152\n",
            "find good         9.198424\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frugal after COVID"
      ],
      "metadata": {
        "id": "K9frRTtvMYtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzqg6jjHoTKo",
        "outputId": "4b2ff823-34fe-42ae-db5e-25db90884b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "buy      55.607722\n",
            "use      43.519472\n",
            "year     42.682866\n",
            "car      40.023208\n",
            "good     36.893904\n",
            "time     33.822442\n",
            "look     33.532132\n",
            "pay      33.163801\n",
            "food     32.982746\n",
            "thing    30.605548\n",
            "think    30.072742\n",
            "need     29.853243\n",
            "store    29.755777\n",
            "lot      29.478572\n",
            "know     29.245842\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "year ago         8.007792\n",
            "save money       7.656282\n",
            "grocery store    7.266219\n",
            "buy new          7.169520\n",
            "year old         5.991183\n",
            "use car          5.750606\n",
            "new car          5.091808\n",
            "thrift store     4.300162\n",
            "buy thing        4.232861\n",
            "spend money      4.116472\n",
            "good deal        4.104385\n",
            "lot money        3.817864\n",
            "long run         3.779976\n",
            "high quality     3.629798\n",
            "buy house        3.434851\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VsoBWZnO2Xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}