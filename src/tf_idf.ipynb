{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN15bvsM0vj6Gvw+nFgXhZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws5i3gpaUzKk",
        "outputId": "f250fbda-9547-47c3-a372-81099f6bb1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hpUFhadVe4r",
        "outputId": "307015d3-476d-4f95-8b2d-18a1365958f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m156.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the TSV file\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_food_results_with_year_month.tsv'\n",
        "file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_food_results_20260819_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_cars_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_RealEstate_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_travel_results_with_year_month.tsv'\n",
        "# file_path = '/content/drive/MyDrive/world-inflation/result/tsv/merged_Frugal_results_with_year_month.tsv'\n",
        "\n",
        "# Read the TSV file into a DataFrame\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "p3B10ksHU1JH",
        "outputId": "170aed20-93ff-4786-83a6-c2d337d4dd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              created_date subreddit_id      id      author parent_id  \\\n",
              "97363  2017-10-17 15:58:43     t5_2qh55  76ype9    NOLAnews       NaN   \n",
              "97364  2020-03-09 12:55:36     t5_2qh55  fftj2h  Dan_Ahdoot       NaN   \n",
              "97365  2020-07-11 18:12:44     t5_2qh55  hpclpl    squid50s       NaN   \n",
              "97366  2022-04-01 22:08:08     t5_2qh55  tu06ky   Sun_Beams       NaN   \n",
              "97367  2022-08-03 21:49:52     t5_2qh55  wfg5zr   Sun_Beams       NaN   \n",
              "\n",
              "                                                    body  score  inflation  \\\n",
              "97363  Hello, Reddit! My name is Todd Price and I’m t...     39          1   \n",
              "97364  What does the food your food say about you? Ha...     53          1   \n",
              "97365  Welcome to [r/food](https://www.reddit.com/r/f...    123          1   \n",
              "97366  Hello and thank you to everyone who took part ...     10          1   \n",
              "97367  ###With summer Reddit in full swing we're gett...     36          1   \n",
              "\n",
              "      year_month  \n",
              "97363    2017-10  \n",
              "97364    2020-03  \n",
              "97365    2020-07  \n",
              "97366    2022-04  \n",
              "97367    2022-08  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e79d54ab-248d-4512-b913-f8ab5eda6326\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_date</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>inflation</th>\n",
              "      <th>year_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97363</th>\n",
              "      <td>2017-10-17 15:58:43</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>76ype9</td>\n",
              "      <td>NOLAnews</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello, Reddit! My name is Todd Price and I’m t...</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97364</th>\n",
              "      <td>2020-03-09 12:55:36</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>fftj2h</td>\n",
              "      <td>Dan_Ahdoot</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What does the food your food say about you? Ha...</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97365</th>\n",
              "      <td>2020-07-11 18:12:44</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hpclpl</td>\n",
              "      <td>squid50s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to [r/food](https://www.reddit.com/r/f...</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97366</th>\n",
              "      <td>2022-04-01 22:08:08</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>tu06ky</td>\n",
              "      <td>Sun_Beams</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello and thank you to everyone who took part ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97367</th>\n",
              "      <td>2022-08-03 21:49:52</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>wfg5zr</td>\n",
              "      <td>Sun_Beams</td>\n",
              "      <td>NaN</td>\n",
              "      <td>###With summer Reddit in full swing we're gett...</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e79d54ab-248d-4512-b913-f8ab5eda6326')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e79d54ab-248d-4512-b913-f8ab5eda6326 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e79d54ab-248d-4512-b913-f8ab5eda6326');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a9c12696-dd81-400f-a4db-a3728bc1808a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9c12696-dd81-400f-a4db-a3728bc1808a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a9c12696-dd81-400f-a4db-a3728bc1808a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# food\n",
        "# filtered_df = df[(df['year_month'] >= '2016-12') & (df['year_month'] <= '2020-05') & (df['inflation'] == 2)]\n",
        "filtered_df = df[(df['year_month'] >= '2020-06') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# cars\n",
        "# filtered_df = df[(df['year_month'] >= '2018-07') & (df['year_month'] <= '2020-11') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-11') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# Real Estate\n",
        "# filtered_df = df[(df['year_month'] >= '2015-05') & (df['year_month'] <= '2020-07') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-08') & (df['year_month'] <= '2022-07') & (df['inflation'] == 2)]\n",
        "# Travel\n",
        "# filtered_df = df[(df['year_month'] >= '2019-02') & (df['year_month'] <= '2020-06') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-07') & (df['year_month'] <= '2022-12') & (df['inflation'] == 2)]\n",
        "# Frugal\n",
        "# filtered_df = df[(df['year_month'] >= '2015-01') & (df['year_month'] <= '2020-10') & (df['inflation'] == 2)]\n",
        "# filtered_df = df[(df['year_month'] >= '2020-11') & (df['year_month'] <= '2022-04') & (df['inflation'] == 2)]\n",
        "filtered_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tgBd4VCeVBxe",
        "outputId": "f577dbf7-af26-4a3d-a92d-5e0c550ffc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              created_date subreddit_id       id                author  \\\n",
              "5153   2022-01-01 01:40:23     t5_2qh55  hqr052e          SatansBarber   \n",
              "5163   2022-01-01 08:43:02     t5_2qh55  hqs941p            mastamaker   \n",
              "5165   2022-01-01 10:53:46     t5_2qh55  hqsibio           lacrosse771   \n",
              "5166   2022-01-01 10:58:23     t5_2qh55  hqsimsx              NedosEUW   \n",
              "5172   2022-01-01 16:33:38     t5_2qh55  hqtaw7m       thecookingofjoy   \n",
              "...                    ...          ...      ...                   ...   \n",
              "95706  2021-12-30 04:38:28     t5_2qh55  hqi2p8z        1993camrywagon   \n",
              "95708  2021-12-30 05:00:08     t5_2qh55  hqi5hg4       tinymongoose909   \n",
              "95713  2021-12-30 06:17:38     t5_2qh55  hqieogp           Hunk_n_Butt   \n",
              "95719  2021-12-30 15:07:03     t5_2qh55  hqjpanw  Mysterious_Main_5391   \n",
              "95725  2021-12-30 21:18:40     t5_2qh55  hql8u9o            SirWernich   \n",
              "\n",
              "        parent_id                                               body  score  \\\n",
              "5153   t1_hqqzs2s  I hope it tastes good. If not, still protein l...      1   \n",
              "5163   t1_hqs5c8c  Exactly what part of the title is hard to beli...      1   \n",
              "5165   t1_hqsgvtk  hmm awesome I spent over a year living in Engl...      1   \n",
              "5166    t3_rt93kc  I wish I could get Lloyd pans for a reasonable...      1   \n",
              "5172   t1_hqt8td5  Prices have gone up the last couple of years, ...     16   \n",
              "...           ...                                                ...    ...   \n",
              "95706  t1_hqhwij9                                        Over priced      2   \n",
              "95708   t3_rriioy  Go to Nicki's if you want a real Chicago style...      2   \n",
              "95713  t1_hqgtlw5  Also overpriced, it’s just a Vienna beef you c...      1   \n",
              "95719  t1_hqj7evo  Portillos used to be legendary. Now it kinda s...      2   \n",
              "95725  t1_hqk3w1d  here in south africa a 250g lurpak costs R78 (...      2   \n",
              "\n",
              "       inflation year_month  \n",
              "5153           2    2022-01  \n",
              "5163           2    2022-01  \n",
              "5165           2    2022-01  \n",
              "5166           2    2022-01  \n",
              "5172           2    2022-01  \n",
              "...          ...        ...  \n",
              "95706          2    2021-12  \n",
              "95708          2    2021-12  \n",
              "95713          2    2021-12  \n",
              "95719          2    2021-12  \n",
              "95725          2    2021-12  \n",
              "\n",
              "[4702 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f864ff78-3fce-414b-8231-8f65c78c053c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_date</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>inflation</th>\n",
              "      <th>year_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5153</th>\n",
              "      <td>2022-01-01 01:40:23</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqr052e</td>\n",
              "      <td>SatansBarber</td>\n",
              "      <td>t1_hqqzs2s</td>\n",
              "      <td>I hope it tastes good. If not, still protein l...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5163</th>\n",
              "      <td>2022-01-01 08:43:02</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqs941p</td>\n",
              "      <td>mastamaker</td>\n",
              "      <td>t1_hqs5c8c</td>\n",
              "      <td>Exactly what part of the title is hard to beli...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5165</th>\n",
              "      <td>2022-01-01 10:53:46</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqsibio</td>\n",
              "      <td>lacrosse771</td>\n",
              "      <td>t1_hqsgvtk</td>\n",
              "      <td>hmm awesome I spent over a year living in Engl...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>2022-01-01 10:58:23</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqsimsx</td>\n",
              "      <td>NedosEUW</td>\n",
              "      <td>t3_rt93kc</td>\n",
              "      <td>I wish I could get Lloyd pans for a reasonable...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5172</th>\n",
              "      <td>2022-01-01 16:33:38</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqtaw7m</td>\n",
              "      <td>thecookingofjoy</td>\n",
              "      <td>t1_hqt8td5</td>\n",
              "      <td>Prices have gone up the last couple of years, ...</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95706</th>\n",
              "      <td>2021-12-30 04:38:28</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqi2p8z</td>\n",
              "      <td>1993camrywagon</td>\n",
              "      <td>t1_hqhwij9</td>\n",
              "      <td>Over priced</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95708</th>\n",
              "      <td>2021-12-30 05:00:08</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqi5hg4</td>\n",
              "      <td>tinymongoose909</td>\n",
              "      <td>t3_rriioy</td>\n",
              "      <td>Go to Nicki's if you want a real Chicago style...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95713</th>\n",
              "      <td>2021-12-30 06:17:38</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqieogp</td>\n",
              "      <td>Hunk_n_Butt</td>\n",
              "      <td>t1_hqgtlw5</td>\n",
              "      <td>Also overpriced, it’s just a Vienna beef you c...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95719</th>\n",
              "      <td>2021-12-30 15:07:03</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hqjpanw</td>\n",
              "      <td>Mysterious_Main_5391</td>\n",
              "      <td>t1_hqj7evo</td>\n",
              "      <td>Portillos used to be legendary. Now it kinda s...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95725</th>\n",
              "      <td>2021-12-30 21:18:40</td>\n",
              "      <td>t5_2qh55</td>\n",
              "      <td>hql8u9o</td>\n",
              "      <td>SirWernich</td>\n",
              "      <td>t1_hqk3w1d</td>\n",
              "      <td>here in south africa a 250g lurpak costs R78 (...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4702 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f864ff78-3fce-414b-8231-8f65c78c053c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f864ff78-3fce-414b-8231-8f65c78c053c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f864ff78-3fce-414b-8231-8f65c78c053c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f9c28868-8b3e-4b3f-a191-8dfbc9c99599\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9c28868-8b3e-4b3f-a191-8dfbc9c99599')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f9c28868-8b3e-4b3f-a191-8dfbc9c99599 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_cc068503-f787-4353-b6d4-05076bca5485\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('filtered_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cc068503-f787-4353-b6d4-05076bca5485 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('filtered_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_df",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English tokenizer from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Extract the 'body' column and apply stemming\n",
        "stemmed_texts = filtered_df['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
        "\n",
        "# Display the first 10 stemmed results\n",
        "for stemmed_text in stemmed_texts.head(10):\n",
        "    print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCZ0lNlVUoX",
        "outputId": "2f05b31d-952c-482a-c87c-d302874df7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I hope it taste good . if not , still protein lol . buy pack of Jack Link 's be crazy expensive so I think let us do thiis !\n",
            "exactly what part of the title be hard to believe ? custom cake be not cheap and depend on where op live it can be incredibly difficult to find a professional baker to make a custom cake on short notice .\n",
            "hmm awesome I spend over a year live in England and travel Europe a few year ago . the only place I see it on a menu be in Dublin and have to buy a very expensive drink with of it to remind of high school .\n",
            "I wish I could get Lloyd pan for a reasonable price here in Germany . they be almost impossible to get .\n",
            "price have go up the last couple of year , but I would not consider this to be an expensive meal ( each dish be about $ 6.50-$7.50 usd ) .\n",
            "be anyone else mad that one single piece of naan now cost $ 4 ? I hate to be * that * guy but it be n’t more than just a few year ago you get three for $ 2 . I love naan , it ’ the only way to eat really good indian food , I just do not like have to spend more on bread than my actual meal . that say , it look amazing , I have a little bit of lamb vindaloo leave over from Thursday , I ’m really excited to eat it for lunch !\n",
            "they ’re more lean and flavorful . at Ted ’s you can get the same size as their steak cut but it ’ a bit more expensive .\n",
            "a lot of food have gain popularity over the year and have jack up their price . a couple of they I can think of be chicken wing and beef brisket . chicken wing have get hugely popular over the year and the price go from 79 cent a pound to over $ 3 a pound . brisket become popular among restaurant and home barbecue and their price have go up a ton over the year , from $ 1.50 per pound a decade ago to almost $ 5 - 6 a pound nowadays .\n",
            "that ’ really expensive for dim sum to I , but it depend on where you live !\n",
            "yeah , the price have almost double since 4 year ago ! but it be for my birthday so I figure it be worth it . FWIW , I know it ’ all relative , but I would consider an expensive meal to be > $ 50 usd and an average dinner to be $ 15 - 20 USD .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Clean the text by removing stopwords, numbers, and noise\n",
        "cleaned_texts = stemmed_texts.apply(\n",
        "    lambda text: ' '.join([token.text for token in nlp(text)\n",
        "                          if not token.is_stop and token.is_alpha and not re.search(r'\\d', token.text)])\n",
        ")\n",
        "\n",
        "# Convert cleaned_texts to a list of strings, as required by CountVectorizer\n",
        "cleaned_texts_list = cleaned_texts.tolist()\n",
        "\n",
        "# Display the first 10 cleaned results (stopword removed, numbers, and noise removed)\n",
        "for cleaned_text in cleaned_texts_list[:10]:  # Use cleaned_texts_list instead\n",
        "    print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IdlzqAKVlZ2",
        "outputId": "f52098ff-0e08-4060-9721-6f0fadc4c6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hope taste good protein lol buy pack Jack Link crazy expensive think let thiis\n",
            "exactly title hard believe custom cake cheap depend op live incredibly difficult find professional baker custom cake short notice\n",
            "hmm awesome spend year live England travel Europe year ago place menu Dublin buy expensive drink remind high school\n",
            "wish Lloyd pan reasonable price Germany impossible\n",
            "price couple year consider expensive meal dish usd\n",
            "mad single piece naan cost hate guy year ago love naan way eat good indian food like spend bread actual meal look amazing little bit lamb vindaloo leave Thursday m excited eat lunch\n",
            "lean flavorful Ted size steak cut bit expensive\n",
            "lot food gain popularity year jack price couple think chicken wing beef brisket chicken wing hugely popular year price cent pound pound brisket popular restaurant home barbecue price ton year pound decade ago pound nowadays\n",
            "expensive dim sum depend live\n",
            "yeah price double year ago birthday figure worth FWIW know relative consider expensive meal usd average dinner USD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Step 1: Create Co-occurrence Matrix\n",
        "# Initialize the CountVectorizer to count word frequencies, excluding common English stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X = vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create the co-occurrence matrix by multiplying the term-document matrix with its transpose\n",
        "co_occurrence_matrix = (X.T * X).toarray()\n",
        "\n",
        "# Convert the co-occurrence matrix into a DataFrame for better readability\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=words, columns=words)\n",
        "print(\"Co-occurrence Matrix:\")\n",
        "print(co_occurrence_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lN7CzLWVndm",
        "outputId": "6b7a7b8a-1b3a-41e7-bb41-fd70039efe04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence Matrix:\n",
            "          aardvark  aback  abalone  abe  aber  aberdeen  ability  abington  \\\n",
            "aardvark         1      0        0    0     0         0        0         0   \n",
            "aback            0      1        0    0     0         0        0         0   \n",
            "abalone          0      0        2    0     0         0        0         0   \n",
            "abe              0      0        0    1     0         0        0         0   \n",
            "aber             0      0        0    0     1         0        0         0   \n",
            "...            ...    ...      ...  ...   ...       ...      ...       ...   \n",
            "붕어빵              0      0        0    0     0         0        0         0   \n",
            "ﾟヮﾟ              0      0        0    0     0         0        0         0   \n",
            "𝘧𝘰𝘳𝘨𝘪𝘷𝘦          0      0        0    0     0         0        0         0   \n",
            "𝘮𝘦               0      0        0    0     0         0        0         0   \n",
            "𝘱𝘭𝘦𝘢𝘴𝘦           0      0        0    0     0         0        0         0   \n",
            "\n",
            "          able  abnormal  ...  氷の怪物  燒包  燒味  路地  鍋貼  붕어빵  ﾟヮﾟ  𝘧𝘰𝘳𝘨𝘪𝘷𝘦  𝘮𝘦  \\\n",
            "aardvark     0         0  ...     0   0   0   0   0    0    0        0   0   \n",
            "aback        0         0  ...     0   0   0   0   0    0    0        0   0   \n",
            "abalone      0         0  ...     0   0   0   0   0    0    0        0   0   \n",
            "abe          0         0  ...     0   0   0   0   0    0    0        0   0   \n",
            "aber         0         0  ...     0   0   0   0   0    0    0        0   0   \n",
            "...        ...       ...  ...   ...  ..  ..  ..  ..  ...  ...      ...  ..   \n",
            "붕어빵          0         0  ...     0   0   0   0   0    1    0        0   0   \n",
            "ﾟヮﾟ          0         0  ...     0   0   0   0   0    0    1        0   0   \n",
            "𝘧𝘰𝘳𝘨𝘪𝘷𝘦      0         0  ...     0   0   0   0   0    0    0        1   1   \n",
            "𝘮𝘦           0         0  ...     0   0   0   0   0    0    0        1   1   \n",
            "𝘱𝘭𝘦𝘢𝘴𝘦       0         0  ...     0   0   0   0   0    0    0        1   1   \n",
            "\n",
            "          𝘱𝘭𝘦𝘢𝘴𝘦  \n",
            "aardvark       0  \n",
            "aback          0  \n",
            "abalone        0  \n",
            "abe            0  \n",
            "aber           0  \n",
            "...          ...  \n",
            "붕어빵            0  \n",
            "ﾟヮﾟ            0  \n",
            "𝘧𝘰𝘳𝘨𝘪𝘷𝘦        1  \n",
            "𝘮𝘦             1  \n",
            "𝘱𝘭𝘦𝘢𝘴𝘦         1  \n",
            "\n",
            "[8561 rows x 8561 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 2: Create TF-IDF Matrix\n",
        "\n",
        "# # List of words to exclude\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# # exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# # exclude_words = ['real', 'real', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'york', 'san', 'los'\n",
        "# ]\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(2, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Gc6oNzV2v5",
        "outputId": "c20e9045-e1e5-4bf9-ce08-76a6c967c1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "                                                    aa buddy  aa love  \\\n",
            "grill break heart chive r start realize expensi...       0.0      0.0   \n",
            "man price wanna weep Vancouver burger range buck         0.0      0.0   \n",
            "DC eat charge plate friend realize pull stack s...       0.0      0.0   \n",
            "pay lb total cost piece change                           0.0      0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...       0.0      0.0   \n",
            "...                                                      ...      ...   \n",
            "use guide turn great super nervous cost protein...       0.0      0.0   \n",
            "check Vegan cheese suck ASS op countess North r...       0.0      0.0   \n",
            "annoying pretty fiddly actually know popular th...       0.0      0.0   \n",
            "good golden rule ve learn saffron add color nec...       0.0      0.0   \n",
            "Almond flour expensive assume                            0.0      0.0   \n",
            "\n",
            "                                                    aa underwhelming  \\\n",
            "grill break heart chive r start realize expensi...               0.0   \n",
            "man price wanna weep Vancouver burger range buck                 0.0   \n",
            "DC eat charge plate friend realize pull stack s...               0.0   \n",
            "pay lb total cost piece change                                   0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...               0.0   \n",
            "...                                                              ...   \n",
            "use guide turn great super nervous cost protein...               0.0   \n",
            "check Vegan cheese suck ASS op countess North r...               0.0   \n",
            "annoying pretty fiddly actually know popular th...               0.0   \n",
            "good golden rule ve learn saffron add color nec...               0.0   \n",
            "Almond flour expensive assume                                    0.0   \n",
            "\n",
            "                                                    aaa angus  aand stiff  \\\n",
            "grill break heart chive r start realize expensi...        0.0         0.0   \n",
            "man price wanna weep Vancouver burger range buck          0.0         0.0   \n",
            "DC eat charge plate friend realize pull stack s...        0.0         0.0   \n",
            "pay lb total cost piece change                            0.0         0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...        0.0         0.0   \n",
            "...                                                       ...         ...   \n",
            "use guide turn great super nervous cost protein...        0.0         0.0   \n",
            "check Vegan cheese suck ASS op countess North r...        0.0         0.0   \n",
            "annoying pretty fiddly actually know popular th...        0.0         0.0   \n",
            "good golden rule ve learn saffron add color nec...        0.0         0.0   \n",
            "Almond flour expensive assume                             0.0         0.0   \n",
            "\n",
            "                                                    aaron franklin  ab dollar  \\\n",
            "grill break heart chive r start realize expensi...             0.0        0.0   \n",
            "man price wanna weep Vancouver burger range buck               0.0        0.0   \n",
            "DC eat charge plate friend realize pull stack s...             0.0        0.0   \n",
            "pay lb total cost piece change                                 0.0        0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...             0.0        0.0   \n",
            "...                                                            ...        ...   \n",
            "use guide turn great super nervous cost protein...             0.0        0.0   \n",
            "check Vegan cheese suck ASS op countess North r...             0.0        0.0   \n",
            "annoying pretty fiddly actually know popular th...             0.0        0.0   \n",
            "good golden rule ve learn saffron add color nec...             0.0        0.0   \n",
            "Almond flour expensive assume                                  0.0        0.0   \n",
            "\n",
            "                                                    ab travel  abalone cup  \\\n",
            "grill break heart chive r start realize expensi...        0.0          0.0   \n",
            "man price wanna weep Vancouver burger range buck          0.0          0.0   \n",
            "DC eat charge plate friend realize pull stack s...        0.0          0.0   \n",
            "pay lb total cost piece change                            0.0          0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...        0.0          0.0   \n",
            "...                                                       ...          ...   \n",
            "use guide turn great super nervous cost protein...        0.0          0.0   \n",
            "check Vegan cheese suck ASS op countess North r...        0.0          0.0   \n",
            "annoying pretty fiddly actually know popular th...        0.0          0.0   \n",
            "good golden rule ve learn saffron add color nec...        0.0          0.0   \n",
            "Almond flour expensive assume                             0.0          0.0   \n",
            "\n",
            "                                                    abalone etc  ...  大董 da  \\\n",
            "grill break heart chive r start realize expensi...          0.0  ...    0.0   \n",
            "man price wanna weep Vancouver burger range buck            0.0  ...    0.0   \n",
            "DC eat charge plate friend realize pull stack s...          0.0  ...    0.0   \n",
            "pay lb total cost piece change                              0.0  ...    0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...          0.0  ...    0.0   \n",
            "...                                                         ...  ...    ...   \n",
            "use guide turn great super nervous cost protein...          0.0  ...    0.0   \n",
            "check Vegan cheese suck ASS op countess North r...          0.0  ...    0.0   \n",
            "annoying pretty fiddly actually know popular th...          0.0  ...    0.0   \n",
            "good golden rule ve learn saffron add color nec...          0.0  ...    0.0   \n",
            "Almond flour expensive assume                               0.0  ...    0.0   \n",
            "\n",
            "                                                    大董烤鸭 good  小龙坎 authentic  \\\n",
            "grill break heart chive r start realize expensi...        0.0            0.0   \n",
            "man price wanna weep Vancouver burger range buck          0.0            0.0   \n",
            "DC eat charge plate friend realize pull stack s...        0.0            0.0   \n",
            "pay lb total cost piece change                            0.0            0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...        0.0            0.0   \n",
            "...                                                       ...            ...   \n",
            "use guide turn great super nervous cost protein...        0.0            0.0   \n",
            "check Vegan cheese suck ASS op countess North r...        0.0            0.0   \n",
            "annoying pretty fiddly actually know popular th...        0.0            0.0   \n",
            "good golden rule ve learn saffron add color nec...        0.0            0.0   \n",
            "Almond flour expensive assume                             0.0            0.0   \n",
            "\n",
            "                                                    涼皮 and肉夹馍  肉夹馍 good  \\\n",
            "grill break heart chive r start realize expensi...        0.0       0.0   \n",
            "man price wanna weep Vancouver burger range buck          0.0       0.0   \n",
            "DC eat charge plate friend realize pull stack s...        0.0       0.0   \n",
            "pay lb total cost piece change                            0.0       0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...        0.0       0.0   \n",
            "...                                                       ...       ...   \n",
            "use guide turn great super nervous cost protein...        0.0       0.0   \n",
            "check Vegan cheese suck ASS op countess North r...        0.0       0.0   \n",
            "annoying pretty fiddly actually know popular th...        0.0       0.0   \n",
            "good golden rule ve learn saffron add color nec...        0.0       0.0   \n",
            "Almond flour expensive assume                             0.0       0.0   \n",
            "\n",
            "                                                    분당구 kind  삼겹살 dude  \\\n",
            "grill break heart chive r start realize expensi...       0.0       0.0   \n",
            "man price wanna weep Vancouver burger range buck         0.0       0.0   \n",
            "DC eat charge plate friend realize pull stack s...       0.0       0.0   \n",
            "pay lb total cost piece change                           0.0       0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...       0.0       0.0   \n",
            "...                                                      ...       ...   \n",
            "use guide turn great super nervous cost protein...       0.0       0.0   \n",
            "check Vegan cheese suck ASS op countess North r...       0.0       0.0   \n",
            "annoying pretty fiddly actually know popular th...       0.0       0.0   \n",
            "good golden rule ve learn saffron add color nec...       0.0       0.0   \n",
            "Almond flour expensive assume                            0.0       0.0   \n",
            "\n",
            "                                                    쌈장 mixed  천겹살 layered  \\\n",
            "grill break heart chive r start realize expensi...       0.0          0.0   \n",
            "man price wanna weep Vancouver burger range buck         0.0          0.0   \n",
            "DC eat charge plate friend realize pull stack s...       0.0          0.0   \n",
            "pay lb total cost piece change                           0.0          0.0   \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...       0.0          0.0   \n",
            "...                                                      ...          ...   \n",
            "use guide turn great super nervous cost protein...       0.0          0.0   \n",
            "check Vegan cheese suck ASS op countess North r...       0.0          0.0   \n",
            "annoying pretty fiddly actually know popular th...       0.0          0.0   \n",
            "good golden rule ve learn saffron add color nec...       0.0          0.0   \n",
            "Almond flour expensive assume                            0.0          0.0   \n",
            "\n",
            "                                                    합정 sell  \n",
            "grill break heart chive r start realize expensi...      0.0  \n",
            "man price wanna weep Vancouver burger range buck        0.0  \n",
            "DC eat charge plate friend realize pull stack s...      0.0  \n",
            "pay lb total cost piece change                          0.0  \n",
            "Non Kobe region Wagyu wagyu kobe beef japanese ...      0.0  \n",
            "...                                                     ...  \n",
            "use guide turn great super nervous cost protein...      0.0  \n",
            "check Vegan cheese suck ASS op countess North r...      0.0  \n",
            "annoying pretty fiddly actually know popular th...      0.0  \n",
            "good golden rule ve learn saffron add color nec...      0.0  \n",
            "Almond flour expensive assume                           0.0  \n",
            "\n",
            "[8978 rows x 103607 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food before COVID"
      ],
      "metadata": {
        "id": "Igi0M8KiPxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTX3xkBIPb9e",
        "outputId": "002cf1a5-1b28-4f83-bae5-9ea2ca71d7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          143.422522\n",
            "good          131.225539\n",
            "pricey         73.566501\n",
            "think          73.119475\n",
            "eat            71.785299\n",
            "place          70.814055\n",
            "pretty         67.808561\n",
            "buy            67.017478\n",
            "worth          66.710840\n",
            "use            66.628557\n",
            "meat           66.495964\n",
            "restaurant     65.199364\n",
            "try            64.921129\n",
            "time           64.413207\n",
            "live           63.462105\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look delicious    21.100400\n",
            "look good         19.692833\n",
            "lobster roll      14.718989\n",
            "bit pricey        14.612192\n",
            "grocery store     14.234269\n",
            "ice cream         12.884045\n",
            "year ago          12.101452\n",
            "shake shack       11.954780\n",
            "pretty good       11.923791\n",
            "little pricey     11.416855\n",
            "look amazing      11.294659\n",
            "want try          10.499790\n",
            "hard find         10.175283\n",
            "kobe beef          9.907465\n",
            "taste good         9.263085\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food after COVID"
      ],
      "metadata": {
        "id": "a-4EwU8YPu5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kihjuW21OfNZ",
        "outputId": "e04475f4-716f-449e-dbb0-3743fa5e05f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          79.397912\n",
            "good          66.175000\n",
            "use           43.273804\n",
            "meat          39.153459\n",
            "eat           36.914857\n",
            "think         36.711093\n",
            "pricey        36.236954\n",
            "time          36.221291\n",
            "place         33.670440\n",
            "restaurant    33.498952\n",
            "buy           33.345404\n",
            "way           33.013601\n",
            "lobster       32.959377\n",
            "worth         32.565316\n",
            "live          31.108129\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look good         12.940388\n",
            "lobster roll      10.114863\n",
            "look delicious     9.221660\n",
            "grocery store      8.973331\n",
            "year ago           8.609246\n",
            "look great         7.706228\n",
            "look amazing       7.515160\n",
            "bit pricey         6.681978\n",
            "hard find          5.399762\n",
            "hot dog            4.858838\n",
            "little pricey      4.800015\n",
            "high quality       4.790216\n",
            "new york           4.500146\n",
            "short rib          4.343338\n",
            "pretty good        4.102249\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cars before COVID"
      ],
      "metadata": {
        "id": "EXxvX3BJbH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxi9D7mlbFez",
        "outputId": "39fe698e-8c27-4d1f-d4d7-e5d250a449a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "car       196.102963\n",
            "buy        70.604764\n",
            "new        67.964061\n",
            "year       61.801178\n",
            "look       61.105670\n",
            "people     59.970622\n",
            "think      59.434780\n",
            "use        58.251591\n",
            "drive      57.197795\n",
            "want       54.505507\n",
            "high       53.545882\n",
            "good       50.425899\n",
            "time       47.036306\n",
            "lot        45.567389\n",
            "know       44.088570\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "new car       22.721342\n",
            "year old      15.208491\n",
            "sport car     12.700061\n",
            "buy new       12.249622\n",
            "use car       12.071384\n",
            "buy car       10.846983\n",
            "brand new     10.810440\n",
            "year ago       9.570674\n",
            "old car        9.305020\n",
            "people buy     9.021110\n",
            "car year       8.767848\n",
            "luxury car     8.006060\n",
            "base model     7.462739\n",
            "car look       6.827017\n",
            "car worth      6.735325\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cars after COVID"
      ],
      "metadata": {
        "id": "hHTufqsPak9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "id": "nojeKxPIWcXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d859ea-0e31-411d-cc37-badb74c5414d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "car        200.507860\n",
            "buy         76.074557\n",
            "new         72.703472\n",
            "people      69.125604\n",
            "year        68.889828\n",
            "use         60.048779\n",
            "drive       59.936499\n",
            "high        58.821536\n",
            "think       56.870891\n",
            "look        56.169809\n",
            "want        55.118786\n",
            "time        50.533413\n",
            "good        49.437796\n",
            "vehicle     49.309129\n",
            "pay         45.887642\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "new car         26.687530\n",
            "use car         19.210675\n",
            "year ago        15.003095\n",
            "sport car       14.992450\n",
            "buy car         13.183247\n",
            "buy new         12.940419\n",
            "year old        12.879925\n",
            "brand new       11.677048\n",
            "people buy      10.547999\n",
            "luxury car       8.501804\n",
            "car car          8.152328\n",
            "car year         8.057843\n",
            "old car          7.963277\n",
            "electric car     7.538813\n",
            "car market       7.506092\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RealEstate before COVID"
      ],
      "metadata": {
        "id": "Wv_2JSqQf1NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# cars\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL7sqarXfy1T",
        "outputId": "3453ac26-2d04-4dae-be4d-fd79d666980a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "house       242.176073\n",
            "home        206.740487\n",
            "market      155.139378\n",
            "buy         151.638467\n",
            "year        150.861808\n",
            "high        139.554748\n",
            "property    132.952090\n",
            "area        132.643980\n",
            "pay         129.690492\n",
            "offer       121.659441\n",
            "sell        120.687299\n",
            "look        120.330552\n",
            "rent        119.200908\n",
            "want        115.951669\n",
            "time        106.300133\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "buy house         44.545499\n",
            "interest rate     34.193197\n",
            "buy home          26.893986\n",
            "year ago          25.726018\n",
            "bay area          24.413026\n",
            "property taxis    21.122941\n",
            "sell house        20.770665\n",
            "hot market        17.785955\n",
            "house sell        17.588880\n",
            "good luck         17.283839\n",
            "single family     15.936882\n",
            "seller market     15.619880\n",
            "long term         15.505065\n",
            "sq ft             14.894357\n",
            "look house        14.758237\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RealEstate after COVID"
      ],
      "metadata": {
        "id": "G6xDAF7DeFKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# cars\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_KvKS8-aYf8",
        "outputId": "5ea7f592-1a3b-4c98-9094-4dd12491d53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "house     163.578109\n",
            "home      146.224377\n",
            "market    110.672748\n",
            "year      107.328061\n",
            "buy       106.374423\n",
            "people     90.402172\n",
            "high       89.368830\n",
            "rate       87.152895\n",
            "offer      86.206788\n",
            "sell       81.365060\n",
            "pay        75.598666\n",
            "area       71.994751\n",
            "want       71.636574\n",
            "rent       71.185480\n",
            "think      68.297565\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "interest rate       38.193469\n",
            "buy house           32.042560\n",
            "year ago            19.712706\n",
            "buy home            18.907245\n",
            "housing market      13.764943\n",
            "house sell          13.012088\n",
            "sell house          12.884586\n",
            "new construction    12.352712\n",
            "offer ask           11.990217\n",
            "property taxis      11.802241\n",
            "home sell           11.282815\n",
            "month ago           10.642864\n",
            "new home             9.996990\n",
            "bay area             9.866553\n",
            "long term            9.838507\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "travel before COVID"
      ],
      "metadata": {
        "id": "hZL7kdEJv0R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "exclude_words = [\n",
        "    'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "    'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "    'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "    'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "    'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "    'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "    'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "    'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "    'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "    'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "    'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "    'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "    'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "    'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "    'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "    'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "    'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "    'baton rouge', 'richmond', 'hialeah',\n",
        "    'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "    'amtrak', 'greyhound', 'interstate', 'san', 'york', 'los'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_TNyoWgvs6t",
        "outputId": "622f8633-c4e0-49f3-cceb-5c875fb3d644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['across', 'ana', 'angeles', 'antonio', 'baton', 'beach', 'canyon', 'carolina', 'central', 'christi', 'chula', 'city', 'clarita', 'corpus', 'dakota', 'dc', 'diego', 'disney', 'el', 'fort', 'francisco', 'grand', 'hampshire', 'island', 'jersey', 'jose', 'las', 'long', 'louis', 'mexico', 'new', 'north', 'orleans', 'park', 'paso', 'paul', 'petersburg', 'rhode', 'rouge', 'salem', 'santa', 'south', 'springs', 'st', 'states', 'the', 'united', 'us', 'vista', 'wayne', 'west', 'winston', 'world', 'worth'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "day       33.859492\n",
            "flight    32.539139\n",
            "city      29.374193\n",
            "time      29.035391\n",
            "trip      26.757787\n",
            "good      25.484383\n",
            "place     25.289780\n",
            "car       25.237215\n",
            "want      24.052174\n",
            "look      22.926905\n",
            "fly       22.281590\n",
            "way       22.263643\n",
            "think     21.734675\n",
            "drive     20.494979\n",
            "people    20.220639\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "rent car         9.630757\n",
            "national park    5.175079\n",
            "spend day        4.751070\n",
            "day trip         4.724252\n",
            "round trip       4.619062\n",
            "new orleans      4.430840\n",
            "grand canyon     4.270544\n",
            "car rental       4.199778\n",
            "east coast       3.825993\n",
            "road trip        3.750314\n",
            "spend time       3.617164\n",
            "west coast       3.572837\n",
            "time year        3.516019\n",
            "big city         3.189684\n",
            "good luck        3.130565\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "travel after COVID"
      ],
      "metadata": {
        "id": "7S3L0dxnvxlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "exclude_words = [\n",
        "    'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "    'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "    'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "    'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "    'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "    'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "    'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "    'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "    'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "    'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "    'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "    'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "    'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "    'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "    'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "    'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "    'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "    'baton rouge', 'richmond', 'hialeah',\n",
        "    'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "    'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "507j7dukeBzx",
        "outputId": "8e6f85ff-7e49-4718-d05b-555284ee1a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['across', 'ana', 'angeles', 'antonio', 'baton', 'beach', 'canyon', 'carolina', 'central', 'christi', 'chula', 'city', 'clarita', 'corpus', 'dakota', 'dc', 'diego', 'disney', 'el', 'fort', 'francisco', 'grand', 'hampshire', 'island', 'jersey', 'jose', 'las', 'long', 'los', 'louis', 'mexico', 'new', 'north', 'orleans', 'park', 'paso', 'paul', 'petersburg', 'rhode', 'rouge', 'salem', 'santa', 'south', 'springs', 'st', 'states', 'the', 'united', 'us', 'vista', 'wayne', 'west', 'winston', 'world', 'worth'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "time      57.451589\n",
            "day       56.442518\n",
            "flight    52.878730\n",
            "city      52.748550\n",
            "place     51.841348\n",
            "trip      51.107792\n",
            "car       50.037077\n",
            "want      47.499021\n",
            "good      46.597402\n",
            "think     41.017668\n",
            "lot       39.575301\n",
            "hotel     39.448018\n",
            "people    38.350848\n",
            "year      38.166532\n",
            "fly       37.897906\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "rent car                 16.328664\n",
            "rental car               12.400245\n",
            "car rental               11.360510\n",
            "national park            10.192015\n",
            "year ago                  9.594561\n",
            "new orleans               8.945887\n",
            "day trip                  8.753072\n",
            "costa rica                7.577532\n",
            "time year                 7.351130\n",
            "west coast                7.338721\n",
            "public transportation     6.764479\n",
            "round trip                6.754204\n",
            "road trip                 6.454885\n",
            "east coast                6.444626\n",
            "good luck                 6.202151\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frugal before COVID"
      ],
      "metadata": {
        "id": "KMP_JHfpMWUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jNgfQ46MQUH",
        "outputId": "9e81a8f8-0c63-41e2-9d00-f79a9923d522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "buy      147.903227\n",
            "use      123.888450\n",
            "car      114.176110\n",
            "year     112.221775\n",
            "good     109.400008\n",
            "pay      107.138028\n",
            "time     101.517401\n",
            "look      94.463426\n",
            "live      94.097667\n",
            "money     91.027264\n",
            "want      90.885079\n",
            "month     90.704992\n",
            "need      90.338385\n",
            "thing     89.535526\n",
            "find      89.297772\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "save money       28.098878\n",
            "grocery store    22.366959\n",
            "year ago         18.651457\n",
            "year old         15.268253\n",
            "buy new          14.539080\n",
            "good deal        13.025636\n",
            "new car          11.538842\n",
            "spend money      11.107503\n",
            "pay month        10.535364\n",
            "long time        10.522145\n",
            "high quality     10.506620\n",
            "lot money        10.108226\n",
            "credit card       9.565503\n",
            "thrift store      9.353152\n",
            "find good         9.198424\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frugal after COVID"
      ],
      "metadata": {
        "id": "K9frRTtvMYtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzqg6jjHoTKo",
        "outputId": "4b2ff823-34fe-42ae-db5e-25db90884b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "buy      55.607722\n",
            "use      43.519472\n",
            "year     42.682866\n",
            "car      40.023208\n",
            "good     36.893904\n",
            "time     33.822442\n",
            "look     33.532132\n",
            "pay      33.163801\n",
            "food     32.982746\n",
            "thing    30.605548\n",
            "think    30.072742\n",
            "need     29.853243\n",
            "store    29.755777\n",
            "lot      29.478572\n",
            "know     29.245842\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "year ago         8.007792\n",
            "save money       7.656282\n",
            "grocery store    7.266219\n",
            "buy new          7.169520\n",
            "year old         5.991183\n",
            "use car          5.750606\n",
            "new car          5.091808\n",
            "thrift store     4.300162\n",
            "buy thing        4.232861\n",
            "spend money      4.116472\n",
            "good deal        4.104385\n",
            "lot money        3.817864\n",
            "long run         3.779976\n",
            "high quality     3.629798\n",
            "buy house        3.434851\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food before COVID"
      ],
      "metadata": {
        "id": "EiIuDm4kQ1EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "id": "8wxxHbQxLYsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7ed9f3-972a-4bd6-d4aa-3e9c19e5d5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          143.422522\n",
            "good          131.225539\n",
            "pricey         73.566501\n",
            "think          73.119475\n",
            "eat            71.785299\n",
            "place          70.814055\n",
            "pretty         67.808561\n",
            "buy            67.017478\n",
            "worth          66.710840\n",
            "use            66.628557\n",
            "meat           66.495964\n",
            "restaurant     65.199364\n",
            "try            64.921129\n",
            "time           64.413207\n",
            "live           63.462105\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look delicious    21.100400\n",
            "look good         19.692833\n",
            "lobster roll      14.718989\n",
            "bit pricey        14.612192\n",
            "grocery store     14.234269\n",
            "ice cream         12.884045\n",
            "year ago          12.101452\n",
            "shake shack       11.954780\n",
            "pretty good       11.923791\n",
            "little pricey     11.416855\n",
            "look amazing      11.294659\n",
            "want try          10.499790\n",
            "hard find         10.175283\n",
            "kobe beef          9.907465\n",
            "taste good         9.263085\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food after COVID"
      ],
      "metadata": {
        "id": "qV0RnUS_Q8O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of words to exclude\n",
        "# food\n",
        "exclude_words = ['food', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# car\n",
        "# exclude_words = ['like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# Real Estate\n",
        "# exclude_words = ['real', 'estate', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "# travel\n",
        "# exclude_words = [\n",
        "#     'travel', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale',\n",
        "#     'america', 'united states', 'usa', 'the us', 'u.s.', 'stateside', 'across the states',\n",
        "#     'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
        "#     'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
        "#     'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
        "#     'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
        "#     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
        "#     'utah', 'vermont', 'virginia', 'washington', ' DC ', 'west virginia', 'wisconsin', 'wyoming',\n",
        "#     'new york', 'ny', 'nyc', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia', 'san antonio', 'san diego',\n",
        "#     'dallas', 'san jose', 'austin', 'jacksonville', 'fort worth', 'columbus', 'indianapolis', 'charlotte', 'san francisco', 'seattle',\n",
        "#     'nashville', 'denver', 'oklahoma city', 'el paso', 'boston', 'portland', 'las vegas', 'vegas', 'detroit', 'memphis', 'louisville',\n",
        "#     'baltimore', 'milwaukee', 'albuquerque', 'tucson', 'fresno', 'sacramento', 'kansas city', 'mesa', 'atlanta', 'omaha',\n",
        "#     'colorado springs', 'raleigh', 'long beach', 'virginia beach', 'miami', 'oakland', 'minneapolis', 'tulsa', 'bakersfield',\n",
        "#     'wichita', 'arlington', 'aurora', 'tampa', 'new orleans', 'cleveland', 'honolulu', 'anaheim', 'lexington', 'stockton',\n",
        "#     'corpus christi', 'henderson', 'riverside', 'newark', 'st. paul', 'santa ana', 'cincinnati', 'irvine', 'orlando', 'pittsburgh',\n",
        "#     'st. louis', 'greensboro', 'jersey city', 'anchorage', 'lincoln', 'plano', 'durham', 'buffalo', 'chandler', 'chula vista',\n",
        "#     'toledo', 'madison', 'gilbert', 'reno', 'fort wayne', 'north las vegas', 'st. petersburg', 'lubbock', 'irving', 'laredo',\n",
        "#     'winston-salem', 'chesapeake', 'glendale', 'garland', 'scottsdale', 'norfolk', 'boise', 'fremont', 'spokane', 'santa clarita',\n",
        "#     'baton rouge', 'richmond', 'hialeah',\n",
        "#     'grand canyon', 'yellowstone', 'hollywood', 'niagara', 'disney world', 'yosemite', 'central park',\n",
        "#     'amtrak', 'greyhound', 'interstate', 'san', 'york'\n",
        "# ]\n",
        "# Frugal\n",
        "# exclude_words = ['frugal', 'like', 'price', 'cost', 'inflation', 'deflation', 'expensive', 'cheap', 'purchase', 'sale']\n",
        "\n",
        "# Initialize the TfidfVectorizer to calculate term frequency-inverse document frequency (TF-IDF)\n",
        "# Use ngram_range=(1, 2) to consider both unigrams (1-word) and bigrams (2-words)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=exclude_words, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on the documents and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_texts_list)\n",
        "\n",
        "# Get the list of words (features) for TF-IDF\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=cleaned_texts_list, columns=tfidf_words)\n",
        "\n",
        "# Function to get top n words for each n-gram type (1-gram and 2-gram)\n",
        "def get_top_n_grams(tfidf_df, n=10):\n",
        "    # Sum the TF-IDF scores across all documents for each n-gram\n",
        "    sum_tfidf = tfidf_df.sum(axis=0)\n",
        "\n",
        "    # Sort the n-grams by their sum of TF-IDF values in descending order\n",
        "    sorted_grams = sum_tfidf.sort_values(ascending=False)\n",
        "\n",
        "    # Return the top n n-grams\n",
        "    return sorted_grams.head(n)\n",
        "\n",
        "# Filter unigrams (1-word) and bigrams (2-words)\n",
        "unigrams = [word for word in tfidf_words if len(word.split()) == 1]\n",
        "bigrams = [word for word in tfidf_words if len(word.split()) == 2]\n",
        "\n",
        "# Get the top 10 unigrams (1-gram)\n",
        "top_1grams = get_top_n_grams(tfidf_df[unigrams], n=15)\n",
        "print(\"Top 15 Unigrams:\")\n",
        "print(top_1grams)\n",
        "\n",
        "# Get the top 10 bigrams (2-gram)\n",
        "top_2grams = get_top_n_grams(tfidf_df[bigrams], n=15)\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "print(top_2grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uenraOAOPXZ0",
        "outputId": "f86a62a4-f8a6-40bf-eb53-943f9e38b5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Unigrams:\n",
            "look          79.397912\n",
            "good          66.175000\n",
            "use           43.273804\n",
            "meat          39.153459\n",
            "eat           36.914857\n",
            "think         36.711093\n",
            "pricey        36.236954\n",
            "time          36.221291\n",
            "place         33.670440\n",
            "restaurant    33.498952\n",
            "buy           33.345404\n",
            "way           33.013601\n",
            "lobster       32.959377\n",
            "worth         32.565316\n",
            "live          31.108129\n",
            "dtype: float64\n",
            "\n",
            "Top 15 Bigrams:\n",
            "look good         12.940388\n",
            "lobster roll      10.114863\n",
            "look delicious     9.221660\n",
            "grocery store      8.973331\n",
            "year ago           8.609246\n",
            "look great         7.706228\n",
            "look amazing       7.515160\n",
            "bit pricey         6.681978\n",
            "hard find          5.399762\n",
            "hot dog            4.858838\n",
            "little pricey      4.800015\n",
            "high quality       4.790216\n",
            "new york           4.500146\n",
            "short rib          4.343338\n",
            "pretty good        4.102249\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VsoBWZnO2Xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}