{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMbam5HYfzEfCjZc+fmK71",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/split_to_training_and_validation_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukNFrNM09aEj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "def create_stratified_samples(df, target_sizes, target_col='inflation'):\n",
        "    \"\"\"\n",
        "    Create stratified samples of different sizes while maintaining class distribution\n",
        "    Each sample is created independently from the original dataset\n",
        "    \"\"\"\n",
        "    # Get the original class distribution\n",
        "    original_dist = df[target_col].value_counts().sort_index()\n",
        "    print(\"Original distribution:\")\n",
        "    total = len(df)\n",
        "    for class_val, count in original_dist.items():\n",
        "        print(f\"Class {class_val}: {count} ({count/total*100:.1f}%)\")\n",
        "\n",
        "    # Store all samples\n",
        "    samples = {}\n",
        "\n",
        "    # Process each target size independently\n",
        "    for i, size in enumerate(target_sizes):\n",
        "        print(f\"\\n{'='*30}\")\n",
        "        print(f\"Creating splitting data of size: {size}\")\n",
        "        print(f\"{'='*30}\")\n",
        "\n",
        "        if size > len(df):\n",
        "            print(f\"Warning: Requested size {size} exceeds total data {len(df)}\")\n",
        "            size = len(df)\n",
        "\n",
        "        # Calculate target counts for each class based on original distribution\n",
        "        target_counts = {}\n",
        "        for class_val in original_dist.index:\n",
        "            target_ratio = original_dist[class_val] / total\n",
        "            target_counts[class_val] = max(1, round(size * target_ratio))\n",
        "\n",
        "        # Adjust if total exceeds target size\n",
        "        while sum(target_counts.values()) > size:\n",
        "            # Reduce the class with the highest count\n",
        "            max_class = max(target_counts.keys(), key=lambda x: target_counts[x])\n",
        "            target_counts[max_class] -= 1\n",
        "\n",
        "        # Ensure total equals target size by adding to classes if needed\n",
        "        while sum(target_counts.values()) < size:\n",
        "            # Add to the class with lowest relative representation\n",
        "            min_class = min(target_counts.keys(), key=lambda x: target_counts[x])\n",
        "            target_counts[min_class] += 1\n",
        "\n",
        "        print(f\"Target counts: {target_counts}\")\n",
        "\n",
        "        # Sample from each class\n",
        "        sample_dfs = []\n",
        "\n",
        "        for class_val, target_count in target_counts.items():\n",
        "            class_data = df[df[target_col] == class_val]\n",
        "            available_count = len(class_data)\n",
        "\n",
        "            if available_count < target_count:\n",
        "                print(f\"Warning: Only {available_count} samples available for class {class_val}, need {target_count}\")\n",
        "                target_count = available_count\n",
        "\n",
        "            if target_count > 0:\n",
        "                # Use different random state for each size to ensure variety\n",
        "                sampled = class_data.sample(n=target_count, random_state=42+i)\n",
        "                sample_dfs.append(sampled)\n",
        "\n",
        "        # Combine and shuffle\n",
        "        if sample_dfs:\n",
        "            sample_df = pd.concat(sample_dfs, ignore_index=True)\n",
        "            # Shuffle with different random state for each sample\n",
        "            sample_df = sample_df.sample(frac=1, random_state=100+i).reset_index(drop=True)\n",
        "            samples[size] = sample_df\n",
        "\n",
        "            # Print distribution for this sample\n",
        "            sample_dist = sample_df[target_col].value_counts().sort_index()\n",
        "            print(f\"Complete dataset size: {len(sample_df)}\")\n",
        "            for class_val, count in sample_dist.items():\n",
        "                print(f\"Class {class_val}: {count} ({count/len(sample_df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"Could not create sample of size {size}\")\n",
        "\n",
        "    return samples\n",
        "\n",
        "def split_and_save_datasets(samples, output_dir):\n",
        "    \"\"\"\n",
        "    Split each dataset into training (75%) and validation (25%) sets and save them\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SPLITTING DATASETS INTO TRAINING AND VALIDATION SETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    training_datasets = {}\n",
        "    validation_datasets = {}\n",
        "\n",
        "    for size, sample_df in samples.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing dataset of size {size}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Check if we have enough samples for stratified split\n",
        "        min_class_count = sample_df['inflation'].value_counts().min()\n",
        "        if min_class_count < 2:\n",
        "            print(f\"Warning: Dataset size {size} has classes with only 1 sample. Cannot perform stratified split.\")\n",
        "            print(\"Performing random split instead...\")\n",
        "            train_df, val_df = train_test_split(\n",
        "                sample_df,\n",
        "                test_size=0.25,\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            # Perform stratified split to maintain class distribution\n",
        "            print(\"Performing stratified split (75% training, 25% validation)...\")\n",
        "            train_df, val_df = train_test_split(\n",
        "                sample_df,\n",
        "                test_size=0.25,\n",
        "                random_state=42,\n",
        "                stratify=sample_df['inflation']\n",
        "            )\n",
        "\n",
        "        # Store the splits\n",
        "        training_datasets[size] = train_df\n",
        "        validation_datasets[size] = val_df\n",
        "\n",
        "        # Print detailed information about the splits\n",
        "        print(f\"\\nOriginal dataset: {len(sample_df)} samples\")\n",
        "        original_dist = sample_df['inflation'].value_counts().sort_index()\n",
        "        for class_val, count in original_dist.items():\n",
        "            print(f\"  Class {class_val}: {count} ({count/len(sample_df)*100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\nTraining set: {len(train_df)} samples ({len(train_df)/len(sample_df)*100:.1f}%)\")\n",
        "        train_dist = train_df['inflation'].value_counts().sort_index()\n",
        "        for class_val, count in train_dist.items():\n",
        "            print(f\"  Class {class_val}: {count} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\nValidation set: {len(val_df)} samples ({len(val_df)/len(sample_df)*100:.1f}%)\")\n",
        "        val_dist = val_df['inflation'].value_counts().sort_index()\n",
        "        for class_val, count in val_dist.items():\n",
        "            print(f\"  Class {class_val}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
        "\n",
        "        # Save training dataset\n",
        "        train_filename = f'training_data_{size}.csv'\n",
        "        train_path = os.path.join(output_dir, train_filename)\n",
        "        train_df.to_csv(train_path, index=False)\n",
        "        print(f\"\\n✓ Saved training data: {train_path}\")\n",
        "        print(f\"  Records: {len(train_df)}\")\n",
        "\n",
        "        # Save validation dataset\n",
        "        val_filename = f'validation_data_{size}.csv'\n",
        "        val_path = os.path.join(output_dir, val_filename)\n",
        "        val_df.to_csv(val_path, index=False)\n",
        "        print(f\"✓ Saved validation data: {val_path}\")\n",
        "        print(f\"  Records: {len(val_df)}\")\n",
        "\n",
        "        # Verify files were saved successfully\n",
        "        if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "            print(f\"✓ File verification: Both files for size {size} successfully saved\")\n",
        "        else:\n",
        "            print(f\"✗ Error: Failed to save files for size {size}\")\n",
        "\n",
        "    return training_datasets, validation_datasets\n",
        "\n",
        "def print_summary(training_datasets, validation_datasets):\n",
        "    \"\"\"\n",
        "    Print a summary of all created datasets\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SUMMARY OF CREATED DATASETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    total_train_files = len(training_datasets)\n",
        "    total_val_files = len(validation_datasets)\n",
        "\n",
        "    print(f\"Total training datasets created: {total_train_files}\")\n",
        "    print(f\"Total validation datasets created: {total_val_files}\")\n",
        "\n",
        "    print(f\"\\n{'Training Datasets:':<25} {'Validation Datasets:'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for size in sorted(training_datasets.keys()):\n",
        "        train_count = len(training_datasets[size])\n",
        "        val_count = len(validation_datasets[size])\n",
        "        print(f\"training_data_{size}.csv ({train_count:>3} samples)   validation_data_{size}.csv ({val_count:>2} samples)\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"STRATIFIED TRAINING DATA GENERATOR\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Read the CSV file\n",
        "    file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/training-validation-main-prod.csv'\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nLoading data from: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"✓ Successfully loaded {len(df)} records\")\n",
        "        print(f\"✓ Columns found: {list(df.columns)}\")\n",
        "\n",
        "        # Verify the data structure\n",
        "        if 'body' not in df.columns or 'inflation' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'body' and 'inflation' not found in the dataset\")\n",
        "\n",
        "        # Check for missing values\n",
        "        body_missing = df['body'].isnull().sum()\n",
        "        inflation_missing = df['inflation'].isnull().sum()\n",
        "        print(f\"Missing values - body: {body_missing}, inflation: {inflation_missing}\")\n",
        "\n",
        "        # Remove any rows with missing values\n",
        "        df_clean = df.dropna(subset=['body', 'inflation'])\n",
        "        print(f\"✓ After removing missing values: {len(df_clean)} records\")\n",
        "\n",
        "        # Define target sizes\n",
        "        target_sizes = [65, 129, 258, 517, 1033]\n",
        "        print(f\"✓ Target dataset sizes: {target_sizes}\")\n",
        "\n",
        "        # Create stratified samples\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"CREATING STRATIFIED SAMPLES\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        samples = create_stratified_samples(df_clean, target_sizes)\n",
        "\n",
        "        # Set output directory\n",
        "        output_dir = '/content/drive/MyDrive/world-inflation/data/reddit/production'\n",
        "        print(f\"\\nOutput directory: {output_dir}\")\n",
        "\n",
        "        # Split datasets and save them\n",
        "        training_datasets, validation_datasets = split_and_save_datasets(samples, output_dir)\n",
        "\n",
        "        # Print summary\n",
        "        print_summary(training_datasets, validation_datasets)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"✓ ALL OPERATIONS COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ Error: Could not find the file {file_path}\")\n",
        "        print(\"Please check if the file path is correct and the file exists.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "hSLXPKA_91A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The number of classes is the same for the training and validation data."
      ],
      "metadata": {
        "id": "9m5te6AriEfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/training-validation-main-prod.csv'\n",
        "\n",
        "# Save the split datasets\n",
        "train_output_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/training_data_954.csv'\n",
        "val_output_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/validation_data_954.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Original class distribution:\\n{df['inflation'].value_counts().sort_index()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# Remove any rows with missing values if they exist\n",
        "df = df.dropna()\n",
        "\n",
        "# Find the minimum class count to balance all classes\n",
        "class_counts = df['inflation'].value_counts()\n",
        "min_class_count = class_counts.min()\n",
        "print(f\"\\nMinimum class count: {min_class_count}\")\n",
        "print(f\"Will balance all classes to {min_class_count} samples each\")\n",
        "\n",
        "# Balance the dataset by sampling equal numbers from each class\n",
        "balanced_dfs = []\n",
        "for class_val in sorted(df['inflation'].unique()):\n",
        "    class_df = df[df['inflation'] == class_val].sample(n=min_class_count, random_state=42)\n",
        "    balanced_dfs.append(class_df)\n",
        "\n",
        "# Combine balanced classes\n",
        "balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
        "\n",
        "# Shuffle the balanced dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nBalanced dataset shape: {balanced_df.shape}\")\n",
        "print(f\"Balanced class distribution:\\n{balanced_df['inflation'].value_counts().sort_index()}\")\n",
        "\n",
        "# Now split into training (75%) and validation (25%) while maintaining equal class counts\n",
        "train_samples_per_class = int(min_class_count * 0.75)\n",
        "val_samples_per_class = min_class_count - train_samples_per_class\n",
        "\n",
        "print(f\"\\nSamples per class in training: {train_samples_per_class}\")\n",
        "print(f\"Samples per class in validation: {val_samples_per_class}\")\n",
        "\n",
        "# Split each class separately to ensure equal counts\n",
        "train_dfs = []\n",
        "val_dfs = []\n",
        "\n",
        "for class_val in sorted(balanced_df['inflation'].unique()):\n",
        "    class_data = balanced_df[balanced_df['inflation'] == class_val].reset_index(drop=True)\n",
        "\n",
        "    # Split this class's data\n",
        "    train_class = class_data.iloc[:train_samples_per_class]\n",
        "    val_class = class_data.iloc[train_samples_per_class:train_samples_per_class + val_samples_per_class]\n",
        "\n",
        "    train_dfs.append(train_class)\n",
        "    val_dfs.append(val_class)\n",
        "\n",
        "# Combine all classes for training and validation\n",
        "train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "\n",
        "# Final shuffle of training and validation sets\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display final statistics\n",
        "print(f\"\\nFinal Training set shape: {train_df.shape}\")\n",
        "print(f\"Training class distribution:\\n{train_df['inflation'].value_counts().sort_index()}\")\n",
        "\n",
        "print(f\"\\nFinal Validation set shape: {val_df.shape}\")\n",
        "print(f\"Validation class distribution:\\n{val_df['inflation'].value_counts().sort_index()}\")\n",
        "\n",
        "# Verify equal class counts\n",
        "print(f\"\\nVerification - All classes have equal counts:\")\n",
        "print(f\"Training set - Class counts are equal: {len(train_df['inflation'].value_counts().unique()) == 1}\")\n",
        "print(f\"Validation set - Class counts are equal: {len(val_df['inflation'].value_counts().unique()) == 1}\")\n",
        "\n",
        "train_df.to_csv(train_output_path, index=False)\n",
        "val_df.to_csv(val_output_path, index=False)\n",
        "\n",
        "print(f\"\\nFiles saved successfully:\")\n",
        "print(f\"Training data: {train_output_path}\")\n",
        "print(f\"Validation data: {val_output_path}\")\n",
        "\n",
        "# Final verification by reading the saved files\n",
        "saved_train = pd.read_csv(train_output_path)\n",
        "saved_val = pd.read_csv(val_output_path)\n",
        "\n",
        "print(f\"\\nFinal verification from saved files:\")\n",
        "print(f\"Training file - Shape: {saved_train.shape}, Class counts: {saved_train['inflation'].value_counts().sort_index().tolist()}\")\n",
        "print(f\"Validation file - Shape: {saved_val.shape}, Class counts: {saved_val['inflation'].value_counts().sort_index().tolist()}\")"
      ],
      "metadata": {
        "id": "a9GMpdzXC3c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The number of classes is the same for the test data."
      ],
      "metadata": {
        "id": "2DkMeto8pwS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test.csv'\n",
        "\n",
        "# Save the balanced and shuffled data as \"test-prod-180.csv\"\n",
        "output_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-prod-180.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(df['inflation'].value_counts().sort_index())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Balance the classes - ensure each class has the same number of samples\n",
        "class_counts = df['inflation'].value_counts().sort_index()\n",
        "print(f\"Original class distribution:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Find the minimum class count to balance all classes\n",
        "min_count = class_counts.min()\n",
        "print(f\"\\nMinimum class count: {min_count}\")\n",
        "print(f\"Balancing all classes to {min_count} samples each\")\n",
        "\n",
        "# Sample equal number of instances from each class\n",
        "balanced_dfs = []\n",
        "for class_label in sorted(df['inflation'].unique()):\n",
        "    class_df = df[df['inflation'] == class_label]\n",
        "    sampled_df = class_df.sample(n=min_count, random_state=42)\n",
        "    balanced_dfs.append(sampled_df)\n",
        "\n",
        "# Combine all balanced classes\n",
        "balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
        "\n",
        "# Shuffle the balanced dataset\n",
        "shuffled_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nBalanced and shuffled dataset shape: {shuffled_df.shape}\")\n",
        "print(f\"Class distribution after balancing:\")\n",
        "print(shuffled_df['inflation'].value_counts().sort_index())\n",
        "\n",
        "shuffled_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nBalanced data saved to: {output_path}\")\n",
        "\n",
        "# Display final statistics\n",
        "print(f\"\\nFinal Statistics:\")\n",
        "print(f\"Original total samples: {len(df)}\")\n",
        "print(f\"Balanced total samples: {len(shuffled_df)}\")\n",
        "print(f\"Samples per class: {min_count}\")\n",
        "print(f\"Total classes: {len(df['inflation'].unique())}\")\n",
        "print(f\"Data successfully balanced, shuffled, and saved!\")"
      ],
      "metadata": {
        "id": "fjQd7c5aqKw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split training, validation, and test data."
      ],
      "metadata": {
        "id": "Yn-uPS3rMC18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(df['inflation'].value_counts().sort_index())\n",
        "print(f\"Class proportions:\")\n",
        "original_props = df['inflation'].value_counts(normalize=True).sort_index()\n",
        "print(original_props)\n",
        "\n",
        "# 2. Verify the data structure\n",
        "print(f\"\\nDataset columns: {list(df.columns)}\")\n",
        "print(f\"Total records: {len(df)}\")\n",
        "\n",
        "# 3. Shuffle the dataset\n",
        "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(f\"\\nDataset shuffled successfully\")\n",
        "\n",
        "# 4. Calculate exact number of samples per class for each split\n",
        "# Test data: 200 records\n",
        "# Remaining: 1043 records split into training (75%) and validation (25%)\n",
        "# Training: 1043 * 0.75 = 782.25 ≈ 782 records\n",
        "# Validation: 1043 * 0.25 = 260.75 ≈ 261 records\n",
        "\n",
        "test_size = 200\n",
        "remaining_size = len(df_shuffled) - test_size\n",
        "train_size = int(remaining_size * 0.75)\n",
        "val_size = remaining_size - train_size\n",
        "\n",
        "print(f\"\\nTarget sizes:\")\n",
        "print(f\"Training: {train_size}\")\n",
        "print(f\"Validation: {val_size}\")\n",
        "print(f\"Test: {test_size}\")\n",
        "\n",
        "# Calculate samples per class for each split to maintain identical ratios\n",
        "class_counts = df_shuffled['inflation'].value_counts().sort_index()\n",
        "total_samples = len(df_shuffled)\n",
        "\n",
        "# For each split, calculate how many samples of each class we need\n",
        "def calculate_class_samples(target_size, class_counts, total_samples):\n",
        "    class_samples = {}\n",
        "    remaining_samples = target_size\n",
        "\n",
        "    # Calculate proportional samples for each class\n",
        "    for class_label in sorted(class_counts.index):\n",
        "        if class_label == sorted(class_counts.index)[-1]:  # Last class gets remaining samples\n",
        "            class_samples[class_label] = remaining_samples\n",
        "        else:\n",
        "            proportion = class_counts[class_label] / total_samples\n",
        "            samples = int(round(target_size * proportion))\n",
        "            class_samples[class_label] = samples\n",
        "            remaining_samples -= samples\n",
        "\n",
        "    return class_samples\n",
        "\n",
        "train_class_samples = calculate_class_samples(train_size, class_counts, total_samples)\n",
        "val_class_samples = calculate_class_samples(val_size, class_counts, total_samples)\n",
        "test_class_samples = calculate_class_samples(test_size, class_counts, total_samples)\n",
        "\n",
        "print(f\"\\nSamples per class:\")\n",
        "print(f\"Training: {train_class_samples}\")\n",
        "print(f\"Validation: {val_class_samples}\")\n",
        "print(f\"Test: {test_class_samples}\")\n",
        "\n",
        "# 5. Create stratified splits with identical ratios\n",
        "def create_split_with_exact_ratios(df, class_samples, split_name):\n",
        "    split_data = []\n",
        "\n",
        "    for class_label, num_samples in class_samples.items():\n",
        "        class_data = df[df['inflation'] == class_label]\n",
        "        if len(class_data) < num_samples:\n",
        "            print(f\"Warning: Not enough samples for class {class_label} in {split_name}\")\n",
        "            selected_samples = class_data\n",
        "        else:\n",
        "            selected_samples = class_data.sample(n=num_samples, random_state=42)\n",
        "        split_data.append(selected_samples)\n",
        "\n",
        "    return pd.concat(split_data, ignore_index=True)\n",
        "\n",
        "# Create a copy for sampling\n",
        "df_remaining = df_shuffled.copy()\n",
        "\n",
        "# Create test split\n",
        "test_data = create_split_with_exact_ratios(df_remaining, test_class_samples, \"test\")\n",
        "# Remove test samples from remaining data\n",
        "df_remaining = df_remaining.drop(test_data.index).reset_index(drop=True)\n",
        "\n",
        "# Create training split from remaining data\n",
        "training_data = create_split_with_exact_ratios(df_remaining, train_class_samples, \"training\")\n",
        "# Remove training samples from remaining data\n",
        "df_remaining = df_remaining.drop(training_data.index).reset_index(drop=True)\n",
        "\n",
        "# Create validation split from remaining data\n",
        "validation_data = create_split_with_exact_ratios(df_remaining, val_class_samples, \"validation\")\n",
        "\n",
        "# Shuffle each split\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "validation_data = validation_data.sample(frac=1, random_state=43).reset_index(drop=True)\n",
        "test_data = test_data.sample(frac=1, random_state=44).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nFinal split sizes:\")\n",
        "print(f\"Training data shape: {training_data.shape}\")\n",
        "print(f\"Validation data shape: {validation_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "# Verify class distributions\n",
        "print(f\"\\nClass distributions:\")\n",
        "print(f\"Training data:\")\n",
        "train_counts = training_data['inflation'].value_counts().sort_index()\n",
        "train_props = training_data['inflation'].value_counts(normalize=True).sort_index()\n",
        "print(f\"  Counts: {dict(train_counts)}\")\n",
        "print(f\"  Proportions: {dict(train_props)}\")\n",
        "\n",
        "print(f\"\\nValidation data:\")\n",
        "val_counts = validation_data['inflation'].value_counts().sort_index()\n",
        "val_props = validation_data['inflation'].value_counts(normalize=True).sort_index()\n",
        "print(f\"  Counts: {dict(val_counts)}\")\n",
        "print(f\"  Proportions: {dict(val_props)}\")\n",
        "\n",
        "print(f\"\\nTest data:\")\n",
        "test_counts = test_data['inflation'].value_counts().sort_index()\n",
        "test_props = test_data['inflation'].value_counts(normalize=True).sort_index()\n",
        "print(f\"  Counts: {dict(test_counts)}\")\n",
        "print(f\"  Proportions: {dict(test_props)}\")\n",
        "\n",
        "# 6. Save the datasets\n",
        "output_dir = '/content/drive/MyDrive/world-inflation/data/reddit/production/'\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save files\n",
        "training_data.to_csv(os.path.join(output_dir, 'training-data-1243.csv'), index=False)\n",
        "validation_data.to_csv(os.path.join(output_dir, 'validation-data-1243.csv'), index=False)\n",
        "test_data.to_csv(os.path.join(output_dir, 'test-data-1243.csv'), index=False)\n",
        "\n",
        "print(f\"\\nFiles saved successfully:\")\n",
        "print(f\"- Training data: {training_data.shape[0]} records\")\n",
        "print(f\"- Validation data: {validation_data.shape[0]} records\")\n",
        "print(f\"- Test data: {test_data.shape[0]} records\")\n",
        "print(f\"- Total: {training_data.shape[0] + validation_data.shape[0] + test_data.shape[0]} records\")\n",
        "\n",
        "# Final verification - check if proportions are identical\n",
        "print(f\"\\nProportion verification (should be identical):\")\n",
        "print(f\"Training proportions: {[f'{x:.4f}' for x in train_props.values]}\")\n",
        "print(f\"Validation proportions: {[f'{x:.4f}' for x in val_props.values]}\")\n",
        "print(f\"Test proportions: {[f'{x:.4f}' for x in test_props.values]}\")\n",
        "\n",
        "# Check maximum difference between proportions\n",
        "max_diff_train_val = max(abs(train_props - val_props))\n",
        "max_diff_train_test = max(abs(train_props - test_props))\n",
        "max_diff_val_test = max(abs(val_props - test_props))\n",
        "\n",
        "print(f\"\\nMaximum proportion differences:\")\n",
        "print(f\"Training vs Validation: {max_diff_train_val:.6f}\")\n",
        "print(f\"Training vs Test: {max_diff_train_test:.6f}\")\n",
        "print(f\"Validation vs Test: {max_diff_val_test:.6f}\")"
      ],
      "metadata": {
        "id": "ORSJDprpqgEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9_Ml2eCM6oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}