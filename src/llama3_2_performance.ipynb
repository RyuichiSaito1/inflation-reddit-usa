{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOMEzUIxUxTqzvAJToFuf/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/llama3_2_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using L4 GPU"
      ],
      "metadata": {
        "id": "HyuYbdVR4Gw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQz5HoWf3wIe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.44.0\n",
        "!pip install datasets scikit-learn matplotlib torch torchvision torchaudio\n",
        "!pip install accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "ol4OCI5K35JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning model"
      ],
      "metadata": {
        "id": "K9yaTOtRR5aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1,050"
      ],
      "metadata": {
        "id": "a3UPERT-quAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the tokenizer for Llama\n",
        "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B')\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Encode the test data using formatted_body\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Initialize the fine-tuned Llama model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        '/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning/checkpoint-144/',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)  # Smaller batch size for Llama\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "\n",
        "    # Use the model to predict the labels of the test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            predicted_labels.extend(predictions.cpu().tolist())\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"Processed {(batch_idx + 1) * 8} samples...\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "    print(\"Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "3CZmbGnF4GIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 520"
      ],
      "metadata": {
        "id": "mouwrQ9brJIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the tokenizer for Llama\n",
        "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B')\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Encode the test data using formatted_body\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Initialize the fine-tuned Llama model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        '/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-520/checkpoint-96/',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)  # Smaller batch size for Llama\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "\n",
        "    # Use the model to predict the labels of the test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            predicted_labels.extend(predictions.cpu().tolist())\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"Processed {(batch_idx + 1) * 8} samples...\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "    print(\"Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "xwwltBvRqzRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 260"
      ],
      "metadata": {
        "id": "2AUDK8u12kSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the tokenizer for Llama\n",
        "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B')\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Encode the test data using formatted_body\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Initialize the fine-tuned Llama model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        '/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-260/checkpoint-36/',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)  # Smaller batch size for Llama\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "\n",
        "    # Use the model to predict the labels of the test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            predicted_labels.extend(predictions.cpu().tolist())\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"Processed {(batch_idx + 1) * 8} samples...\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "    print(\"Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "_BcSgXREzKfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 130"
      ],
      "metadata": {
        "id": "iJNSY54t2nJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the tokenizer for Llama\n",
        "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B')\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Encode the test data using formatted_body\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Initialize the fine-tuned Llama model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        '/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-130/checkpoint-24/',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)  # Smaller batch size for Llama\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "\n",
        "    # Use the model to predict the labels of the test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            predicted_labels.extend(predictions.cpu().tolist())\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"Processed {(batch_idx + 1) * 8} samples...\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "    print(\"Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "4dfSR9vh2uiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 65"
      ],
      "metadata": {
        "id": "Ud_uJbT6_2wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the tokenizer for Llama\n",
        "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B')\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Encode the test data using formatted_body\n",
        "    test_encodings = tokenizer(\n",
        "        test_data['formatted_body'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = TestDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Initialize the fine-tuned Llama model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        '/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-65/checkpoint-12/',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)  # Smaller batch size for Llama\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "\n",
        "    # Use the model to predict the labels of the test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            predicted_labels.extend(predictions.cpu().tolist())\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"Processed {(batch_idx + 1) * 8} samples...\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "    print(\"Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "kAzksboT_AEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot model"
      ],
      "metadata": {
        "id": "oOGe6OpBdw-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, names=['body', 'inflation'], header=0, dtype='object')\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the IMF economist prompt (same as used in training)\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"\n",
        "    Extract classification from model response.\n",
        "    Looks for patterns like \"Classification: 0\", \"0\", \"2:\", etc.\n",
        "    \"\"\"\n",
        "    # Remove the input prompt to focus on the generated response\n",
        "    if \"Classification:\" in response_text:\n",
        "        response_part = response_text.split(\"Classification:\")[-1].strip()\n",
        "    else:\n",
        "        response_part = response_text\n",
        "\n",
        "    # Look for explicit classification patterns\n",
        "    patterns = [\n",
        "        r'Classification:\\s*([012])',\n",
        "        r'^([012])(?:\\s|$|\\.)',\n",
        "        r'\\b([012])\\b',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_part)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "\n",
        "    # If no clear pattern found, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_part:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    # Default to 1 (neutral) if no classification found\n",
        "    return 1\n",
        "\n",
        "def generate_prediction(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Generate prediction using the model in zero-shot setting.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.1,  # Low temperature for more deterministic outputs\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part (excluding the input prompt)\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "AmGvQ0Ac43vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data file path\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "# Read data from CSV file\n",
        "test_data = read_csv_file(file_path)\n",
        "\n",
        "if test_data is not None:\n",
        "    print(f\"Loaded {len(test_data)} test samples\")\n",
        "\n",
        "    # Format test data with the same prompt used during training\n",
        "    test_data['formatted_body'] = test_data['body'].apply(format_with_prompt)\n",
        "\n",
        "    # Initialize the base Llama model and tokenizer\n",
        "    model_name = 'meta-llama/Llama-3.2-3B'\n",
        "    print(f\"Loading base model: {model_name}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the string labels to integers\n",
        "    test_labels = [int(label) for label in test_data['inflation']]\n",
        "\n",
        "    # Lists to store true and predicted labels\n",
        "    true_labels = test_labels\n",
        "    predicted_labels = []\n",
        "\n",
        "    print(\"Starting zero-shot evaluation...\")\n",
        "\n",
        "    # Process each sample individually for zero-shot inference\n",
        "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing samples\"):\n",
        "        prompt = row['formatted_body']\n",
        "\n",
        "        # Generate prediction\n",
        "        response = generate_prediction(model, tokenizer, prompt)\n",
        "\n",
        "        # Extract classification from response\n",
        "        prediction = extract_classification(response)\n",
        "        predicted_labels.append(prediction)\n",
        "\n",
        "        # Print first few examples for debugging\n",
        "        if idx < 5:\n",
        "            print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "            print(f\"True label: {test_labels[idx]}\")\n",
        "            print(f\"Generated response: {response[:100]}...\")\n",
        "            print(f\"Extracted prediction: {prediction}\")\n",
        "\n",
        "    # Calculate and display accuracy, recall, precision, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "    # Display classification report and confusion matrix\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ZERO-SHOT EVALUATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # Display metrics for each class and macro/micro averages\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(3):\n",
        "        if i < len(recall):\n",
        "            print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "        else:\n",
        "            print(f\"| Class {i}      |    {accuracy:.2f}   |   0.00   |   0.00   |   0.00   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    # Calculate macro averages (handling potential empty arrays)\n",
        "    macro_recall = recall.mean() if len(recall) > 0 else 0.0\n",
        "    macro_precision = precision.mean() if len(precision) > 0 else 0.0\n",
        "    macro_f1 = f1.mean() if len(f1) > 0 else 0.0\n",
        "\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {macro_recall:.2f}   |   {macro_precision:.2f}   |   {macro_f1:.2f}   |\")\n",
        "\n",
        "    # Calculate micro averages\n",
        "    micro_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {micro_recall:.2f}   |   {micro_precision:.2f}   |   {micro_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "\n",
        "    print(f\"\\nTotal test samples: {len(true_labels)}\")\n",
        "\n",
        "    # Print distribution of predictions\n",
        "    from collections import Counter\n",
        "    true_dist = Counter(true_labels)\n",
        "    pred_dist = Counter(predicted_labels)\n",
        "\n",
        "    print(\"\\nLabel Distribution:\")\n",
        "    print(f\"True labels:      {dict(true_dist)}\")\n",
        "    print(f\"Predicted labels: {dict(pred_dist)}\")\n",
        "\n",
        "    print(\"\\nZero-shot evaluation completed successfully!\")\n",
        "    print(\"\\nNote: This uses the base Llama-3.2-3B model without fine-tuning.\")\n",
        "    print(\"Compare these results with your fine-tuned model performance.\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to load test data. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "th_08g9ER_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDJYlnuySwVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}