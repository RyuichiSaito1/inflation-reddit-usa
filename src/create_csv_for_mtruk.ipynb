{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7i3dtd+SabobVcT2rf0iB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/create_csv_for_mtruk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHeVP4xIW7x4",
        "outputId": "fb6387cd-b497-41e7-c8ae-558bf066ec56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Load 'mturk_qualification.tsv' and shuffle\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/training-main-1020.tsv'\n",
        "try:\n",
        "    text_df = pd.read_csv(file_path, sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "text_df = text_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 2: Remove excess whitespace from the 'body' column\n",
        "print(\"Cleaning whitespace from the 'body' column...\")\n",
        "text_df['body'] = text_df['body'].astype(str).apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "print(\"Whitespace cleaning complete.\")\n",
        "\n",
        "\n",
        "# Step 3: Save the cleaned and shuffled DataFrame\n",
        "output_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-1020_shuffled.tsv'\n",
        "try:\n",
        "    text_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "    print(f\"DataFrame was successfully saved to '{output_file_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the file: {e}\")\n",
        "\n",
        "# Step 4: Extract the 'body' column and rename it as 'text' for further processing\n",
        "output_df = text_df[['body']].rename(columns={'body': 'text'})\n",
        "\n",
        "# Step 5: Divide into the top 200 records and the rest\n",
        "if len(output_df) < 200:\n",
        "    print(\"Warning: The total number of records is less than 200.\")\n",
        "    print(\"All records will be saved to the 'top 200' file, and the 'rest' file will be empty.\")\n",
        "    top_200_df = output_df.copy()\n",
        "    rest_df = pd.DataFrame(columns=output_df.columns) # Empty dataframe with same columns\n",
        "else:\n",
        "    top_200_df = output_df.head(200)\n",
        "    rest_df = output_df.iloc[200:]\n",
        "\n",
        "# Step 6: Store each file\n",
        "top_200_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-top200.csv'\n",
        "rest_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-rest.csv'\n",
        "\n",
        "top_200_df.to_csv(top_200_file_path, index=False)\n",
        "rest_df.to_csv(rest_file_path, index=False)\n",
        "\n",
        "# Step 7: Print information about the created files\n",
        "print(\"Cleaned Data Information:\")\n",
        "print(f\"First 5 rows of the top 200 records (from '{top_200_file_path}'):\")\n",
        "print(top_200_df.head(5))\n",
        "print(f\"Total number of rows in top 200 records file: {len(top_200_df)}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of the rest of the records (from '{rest_file_path}'):\")\n",
        "if not rest_df.empty:\n",
        "    print(rest_df.head(5))\n",
        "else:\n",
        "    print(\"No remaining records.\")\n",
        "print(f\"Total number of rows in the rest of the records file: {len(rest_df)}\")\n",
        "\n",
        "print(f\"\\nTotal number of rows processed: {len(output_df)}\")\n",
        "print(f\" Processing completed. Files created: \\n   1. '{top_200_file_path}' \\n   2. '{rest_file_path}'\")"
      ],
      "metadata": {
        "id": "c1LlQcLElFwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437c3b7f-84a9-4b1d-ed43-ab289c985392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Cleaning whitespace from the 'body' column...\n",
            "âœ… Whitespace cleaning complete.\n",
            "âœ… DataFrame was successfully saved to '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-1020_shuffled.tsv'\n",
            "\n",
            "ðŸ“Š Cleaned Data Information:\n",
            "First 5 rows of the top 200 records (from '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-top200.csv'):\n",
            "                                                  text\n",
            "523  porsche 944. 24-36mpg, way less complex system...\n",
            "602  The other day when I was parking my '97 Black ...\n",
            "526  I was originally going to use the 2.8 and driv...\n",
            "31   Definitely pay for the lawyer's services. Make...\n",
            "616  Hahaha. It's the salt, butter, and preservativ...\n",
            "Total number of rows in top 200 records file: 200\n",
            "\n",
            "First 5 rows of the rest of the records (from '{rest_file_path}'):\n",
            "                                                  text\n",
            "449  I thought this would be helpful for anyone who...\n",
            "783  Hey reddit, my friends and I were looking to t...\n",
            "593  So my mom wants a rugged terrain vehicle, but ...\n",
            "909  My friends went \"interrailing\" around europe, ...\n",
            "743  >He does one market for filling empty slots in...\n",
            "Total number of rows in the rest of the records file: 820\n",
            "\n",
            "Total number of rows processed: 1020\n",
            "âœ… Processing completed. Files created: \n",
            "   1. '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-top200.csv' \n",
            "   2. '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-rest.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Load the TSV file and shuffle its contents.\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk_qualification.tsv'\n",
        "try:\n",
        "    text_df = pd.read_csv(file_path, sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "text_df = text_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 2: Clean the 'body' column.\n",
        "print(\"Cleaning whitespace from the 'body' column...\")\n",
        "text_df['body'] = text_df['body'].astype(str).apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "print(\"Whitespace cleaning complete.\")\n",
        "\n",
        "\n",
        "# Step 3: Save the cleaned and shuffled DataFrame.\n",
        "output_shuffled_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification_shuffled.tsv'\n",
        "try:\n",
        "    text_df.to_csv(output_shuffled_path, sep='\\t', index=False)\n",
        "    print(f\"Full shuffled DataFrame was successfully saved to '{output_shuffled_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the shuffled file: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: Extract the 'body' column for the final output.\n",
        "output_df = text_df[['body']].rename(columns={'body': 'text'})\n",
        "\n",
        "\n",
        "# Step 5: Save the final single-column data to a single CSV file.\n",
        "output_final_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification.csv'\n",
        "try:\n",
        "    output_df.to_csv(output_final_path, index=False)\n",
        "    print(f\"Final single-column DataFrame was successfully saved to '{output_final_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the final file: {e}\")\n",
        "\n",
        "\n",
        "# Step 6: Print information about the created files.\n",
        "print(\"\\n--- Processing Summary ---\")\n",
        "print(f\"1. Full shuffled data saved to: '{output_shuffled_path}'\")\n",
        "print(f\"2. Final processed data saved to: '{output_final_path}'\")\n",
        "\n",
        "print(\"\\nFinal Processed Data Preview:\")\n",
        "print(f\"First 5 rows of the final records (from '{output_final_path}'):\")\n",
        "print(output_df.head(5))\n",
        "print(f\"Total number of rows processed: {len(output_df)}\")\n",
        "\n",
        "print(\"\\nProcessing completed.\")"
      ],
      "metadata": {
        "id": "vh6MxYXARmUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbca27a-2111-46cd-d120-9555dcec3d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning whitespace from the 'body' column...\n",
            "Whitespace cleaning complete.\n",
            "Full shuffled DataFrame was successfully saved to '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification_shuffled.tsv'\n",
            "Final single-column DataFrame was successfully saved to '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification.csv'\n",
            "\n",
            "--- Processing Summary ---\n",
            "1. Full shuffled data saved to: '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification_shuffled.tsv'\n",
            "2. Final processed data saved to: '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification.csv'\n",
            "\n",
            "Final Processed Data Preview:\n",
            "First 5 rows of the final records (from '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification.csv'):\n",
            "                                                 text\n",
            "9   $2000 is a great price... most of the RTW tick...\n",
            "11  I've always gone from Boston to Dublin/Madrid ...\n",
            "0   Eh, I make fried rice with rice that I've cook...\n",
            "13  OK, one thing that could fuck you over (more o...\n",
            "5   Try to get a later one. You have to be more of...\n",
            "Total number of rows processed: 15\n",
            "\n",
            "Processing completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eU5VuWageKWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}