{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7i3dtd+SabobVcT2rf0iB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/src/create_csv_for_mtruk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHeVP4xIW7x4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Load 'mturk_qualification.tsv' and shuffle\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/training-main-1020.tsv'\n",
        "try:\n",
        "    text_df = pd.read_csv(file_path, sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "text_df = text_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 2: Remove excess whitespace from the 'body' column\n",
        "print(\"Cleaning whitespace from the 'body' column...\")\n",
        "text_df['body'] = text_df['body'].astype(str).apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "print(\"Whitespace cleaning complete.\")\n",
        "\n",
        "\n",
        "# Step 3: Save the cleaned and shuffled DataFrame\n",
        "output_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-1020_shuffled.tsv'\n",
        "try:\n",
        "    text_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "    print(f\"DataFrame was successfully saved to '{output_file_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the file: {e}\")\n",
        "\n",
        "# Step 4: Extract the 'body' column and rename it as 'text' for further processing\n",
        "output_df = text_df[['body']].rename(columns={'body': 'text'})\n",
        "\n",
        "# Step 5: Divide into the top 200 records and the rest\n",
        "if len(output_df) < 200:\n",
        "    print(\"Warning: The total number of records is less than 200.\")\n",
        "    print(\"All records will be saved to the 'top 200' file, and the 'rest' file will be empty.\")\n",
        "    top_200_df = output_df.copy()\n",
        "    rest_df = pd.DataFrame(columns=output_df.columns) # Empty dataframe with same columns\n",
        "else:\n",
        "    top_200_df = output_df.head(200)\n",
        "    rest_df = output_df.iloc[200:]\n",
        "\n",
        "# Step 6: Store each file\n",
        "top_200_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-top200.csv'\n",
        "rest_file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/training-main-rest.csv'\n",
        "\n",
        "top_200_df.to_csv(top_200_file_path, index=False)\n",
        "rest_df.to_csv(rest_file_path, index=False)\n",
        "\n",
        "# Step 7: Print information about the created files\n",
        "print(\"Cleaned Data Information:\")\n",
        "print(f\"First 5 rows of the top 200 records (from '{top_200_file_path}'):\")\n",
        "print(top_200_df.head(5))\n",
        "print(f\"Total number of rows in top 200 records file: {len(top_200_df)}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of the rest of the records (from '{rest_file_path}'):\")\n",
        "if not rest_df.empty:\n",
        "    print(rest_df.head(5))\n",
        "else:\n",
        "    print(\"No remaining records.\")\n",
        "print(f\"Total number of rows in the rest of the records file: {len(rest_df)}\")\n",
        "\n",
        "print(f\"\\nTotal number of rows processed: {len(output_df)}\")\n",
        "print(f\" Processing completed. Files created: \\n   1. '{top_200_file_path}' \\n   2. '{rest_file_path}'\")"
      ],
      "metadata": {
        "id": "c1LlQcLElFwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Load the TSV file and shuffle its contents.\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk_qualification.tsv'\n",
        "try:\n",
        "    text_df = pd.read_csv(file_path, sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "text_df = text_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 2: Clean the 'body' column.\n",
        "print(\"Cleaning whitespace from the 'body' column...\")\n",
        "text_df['body'] = text_df['body'].astype(str).apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "print(\"Whitespace cleaning complete.\")\n",
        "\n",
        "\n",
        "# Step 3: Save the cleaned and shuffled DataFrame.\n",
        "output_shuffled_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification_shuffled.tsv'\n",
        "try:\n",
        "    text_df.to_csv(output_shuffled_path, sep='\\t', index=False)\n",
        "    print(f\"Full shuffled DataFrame was successfully saved to '{output_shuffled_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the shuffled file: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: Extract the 'body' column for the final output.\n",
        "output_df = text_df[['body']].rename(columns={'body': 'text'})\n",
        "\n",
        "\n",
        "# Step 5: Save the final single-column data to a single CSV file.\n",
        "output_final_path = '/content/drive/MyDrive/world-inflation/data/reddit/experiment/mturk/mturk_qualification.csv'\n",
        "try:\n",
        "    output_df.to_csv(output_final_path, index=False)\n",
        "    print(f\"Final single-column DataFrame was successfully saved to '{output_final_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the final file: {e}\")\n",
        "\n",
        "\n",
        "# Step 6: Print information about the created files.\n",
        "print(\"\\n--- Processing Summary ---\")\n",
        "print(f\"1. Full shuffled data saved to: '{output_shuffled_path}'\")\n",
        "print(f\"2. Final processed data saved to: '{output_final_path}'\")\n",
        "\n",
        "print(\"\\nFinal Processed Data Preview:\")\n",
        "print(f\"First 5 rows of the final records (from '{output_final_path}'):\")\n",
        "print(output_df.head(5))\n",
        "print(f\"Total number of rows processed: {len(output_df)}\")\n",
        "\n",
        "print(\"\\nProcessing completed.\")"
      ],
      "metadata": {
        "id": "vh6MxYXARmUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eU5VuWageKWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}