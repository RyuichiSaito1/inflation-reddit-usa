{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/notebooks/gemni_2_0_flash_lite_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x8BiR7gaxVN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvFIyszxa1z7"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install google-genai pandas gspread google-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL70GyEvKdmE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import auth\n",
        "from google import genai\n",
        "from google.genai.types import HttpOptions\n",
        "import gspread\n",
        "from google.auth import default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk3NXGIVKhuo"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Initialize Vertex AI client\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "# Get the tuned model\n",
        "tuning_job_name = \"#####\"\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "model_endpoint = tuning_job.tuned_model.endpoint\n",
        "\n",
        "print(f\"Successfully connected to model: {model_endpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OpkBWm6TTI_"
      },
      "outputs": [],
      "source": [
        "class RedditInflationClassifier:\n",
        "    def __init__(self, model_endpoint, client):\n",
        "        self.model_endpoint = model_endpoint\n",
        "        self.client = client\n",
        "        self.prompt_template = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "\n",
        "    0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "\n",
        "    2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "\n",
        "    1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "\n",
        "    Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post:\n",
        "'{text}'\n",
        "\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "    def create_prompt(self, text):\n",
        "        return self.prompt_template.format(text=text)\n",
        "\n",
        "    def extract_classification(self, response_text):\n",
        "        \"\"\"Extract classification number from model response\"\"\"\n",
        "        patterns = [\n",
        "            r'Classification.*?([012])',\n",
        "            r'Answer.*?([012])',\n",
        "            r'Response.*?([012])',\n",
        "            r'\\b([012])\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, response_text)\n",
        "            if matches:\n",
        "                return int(matches[-1])\n",
        "\n",
        "        # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "        for char in response_text:\n",
        "            if char in ['0', '1', '2']:\n",
        "                return int(char)\n",
        "\n",
        "        return 1  # Default to neutral if unable to extract\n",
        "\n",
        "    def classify_text(self, text):\n",
        "        \"\"\"Classify a single text using the fine-tuned model\"\"\"\n",
        "        try:\n",
        "            prompt = self.create_prompt(text)\n",
        "            response = self.client.models.generate_content(\n",
        "                model=self.model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "            classification = self.extract_classification(response.text)\n",
        "            return classification\n",
        "        except Exception as e:\n",
        "            print(f\"Error in classification: {e}\")\n",
        "            return 1  # Default to neutral on error\n",
        "\n",
        "    def classify_batch(self, texts, delay=0.1):\n",
        "        \"\"\"Classify a batch of texts with rate limiting\"\"\"\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Processing {i+1}/{len(texts)} texts...\")\n",
        "\n",
        "            result = self.classify_text(text)\n",
        "            results.append(result)\n",
        "            time.sleep(delay)  # Rate limiting\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma482P44bcBL"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(file_path, start_date=None, end_date=None):\n",
        "    \"\"\"Load TSV data and prepare for processing with optional date filtering\"\"\"\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "    print(f\"Loaded {len(df)} total records\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Convert created_date to datetime\n",
        "    df['created_date'] = pd.to_datetime(df['created_date'])\n",
        "\n",
        "    # Show original date range\n",
        "    print(f\"Original date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Debug: Show some sample dates before filtering\n",
        "    print(f\"Sample dates from original data:\")\n",
        "    sample_dates = df['created_date'].head(10).tolist()\n",
        "    for i, date in enumerate(sample_dates):\n",
        "        print(f\"  {i+1}: {date}\")\n",
        "\n",
        "    # Filter by date range if specified\n",
        "    if start_date and end_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: {start_date} <= created_date <= {end_date}\")\n",
        "        df = df[(df['created_date'] >= start_date) & (df['created_date'] <= end_date)]\n",
        "        print(f\"Filtered to date range {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif start_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        print(f\"Applying filter: created_date >= {start_date}\")\n",
        "        df = df[df['created_date'] >= start_date]\n",
        "        print(f\"Filtered from {start_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif end_date:\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: created_date <= {end_date}\")\n",
        "        df = df[df['created_date'] <= end_date]\n",
        "        print(f\"Filtered to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "\n",
        "    # Add year_month column for grouping\n",
        "    df['year_month'] = df['created_date'].dt.to_period('M')\n",
        "\n",
        "    print(f\"Final date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Debug: Show the unique year_month values to verify filtering\n",
        "    unique_months = sorted(df['year_month'].unique())\n",
        "    print(f\"Unique year_month values after filtering: {unique_months}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def sample_monthly_data(df, max_samples_per_month=800):\n",
        "    \"\"\"Sample up to max_samples_per_month records per month\"\"\"\n",
        "    sampled_dfs = []\n",
        "\n",
        "    for year_month, group in df.groupby('year_month'):\n",
        "        if len(group) <= max_samples_per_month:\n",
        "            sampled_group = group\n",
        "        else:\n",
        "            sampled_group = group.sample(n=max_samples_per_month, random_state=42)\n",
        "\n",
        "        sampled_dfs.append(sampled_group)\n",
        "        print(f\"{year_month}: sampled {len(sampled_group)} out of {len(group)} records\")\n",
        "\n",
        "    result_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "    print(f\"Total sampled records: {len(result_df)}\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def process_and_classify_data(df, classifier, output_path, spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Process data month by month and classify texts\"\"\"\n",
        "    # Sort by date\n",
        "    df = df.sort_values('created_date')\n",
        "\n",
        "    # Initialize Google Sheets connection\n",
        "    try:\n",
        "        # Use Google Colab authentication\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Get default credentials\n",
        "        from google.auth import default\n",
        "        creds, _ = default()\n",
        "\n",
        "        # Create gspread client\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "        # Open spreadsheet\n",
        "        spreadsheet = gc.open(spreadsheet_name)\n",
        "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
        "\n",
        "        # Check if worksheet has data and add headers if empty\n",
        "        existing_data = worksheet.get_all_values()\n",
        "        if not existing_data:\n",
        "            headers = ['year_month', 'deflation', 'neither', 'inflation', 'total_number', 'average_score']\n",
        "            worksheet.append_row(headers)\n",
        "\n",
        "        print(f\"Connected to existing spreadsheet: {spreadsheet_name} - {worksheet_name}\")\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"Error: Spreadsheet '{spreadsheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Error: Worksheet '{worksheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to Google Sheet: {type(e).__name__}: {str(e)}\")\n",
        "        worksheet = None\n",
        "\n",
        "    # Group by year_month for processing\n",
        "    for year_month, month_group in df.groupby('year_month'):\n",
        "        print(f\"\\nProcessing {year_month}...\")\n",
        "\n",
        "        # Create a copy to avoid modifying original\n",
        "        month_df = month_group.copy()\n",
        "\n",
        "        # Classify texts\n",
        "        texts = month_df['body'].fillna('').astype(str).tolist()\n",
        "        classifications = classifier.classify_batch(texts)\n",
        "\n",
        "        # Add classifications to dataframe\n",
        "        month_df['inflation'] = classifications\n",
        "\n",
        "        # Remove year_month column before saving to TSV\n",
        "        month_df_tsv = month_df.drop('year_month', axis=1)\n",
        "\n",
        "        # Save to TSV file (append mode)\n",
        "        if os.path.exists(output_path):\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='a', header=False, index=False)\n",
        "        else:\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='w', header=True, index=False)\n",
        "\n",
        "        # Calculate monthly summary\n",
        "        deflation_count = len(month_df[month_df['inflation'] == 0])\n",
        "        neither_count = len(month_df[month_df['inflation'] == 1])\n",
        "        inflation_count = len(month_df[month_df['inflation'] == 2])\n",
        "        total_count = len(month_df)\n",
        "        avg_score = (deflation_count * 0 + neither_count * 1 + inflation_count * 2) / total_count\n",
        "\n",
        "        # Output to standard output\n",
        "        print(f\"Month: {year_month}\")\n",
        "        print(f\"  Deflation (0): {deflation_count}\")\n",
        "        print(f\"  Neither (1): {neither_count}\")\n",
        "        print(f\"  Inflation (2): {inflation_count}\")\n",
        "        print(f\"  Total: {total_count}\")\n",
        "        print(f\"  Average Score: {avg_score:.4f}\")\n",
        "\n",
        "        # Add to Google Sheet\n",
        "        if worksheet:\n",
        "            try:\n",
        "                row_data = [str(year_month), deflation_count, neither_count, inflation_count, total_count, round(avg_score, 4)]\n",
        "                worksheet.append_row(row_data)\n",
        "                print(f\"  Added to Google Sheet: {spreadsheet_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error updating Google Sheet: {e}\")\n",
        "\n",
        "        print(f\"Saved {len(month_df)} records for {year_month}\")\n",
        "\n",
        "    print(f\"\\nAll data processed and saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQeQUixfeL7h"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # ☆\n",
        "    # File paths\n",
        "    input_file = \"/content/drive/MyDrive/world-inflation/data/reddit/production/Frugal_submissions_2012_2022.tsv\"\n",
        "    output_file = \"/content/drive/MyDrive/world-inflation/result/tsv/Frugal_submissions_results.tsv\"\n",
        "\n",
        "    # ☆\n",
        "    # Google Sheets configuration - MODIFY THESE VALUES\n",
        "    spreadsheet_name = \"monthly_classification_result\"  # Replace with your spreadsheet name\n",
        "    worksheet_name = \"subfrugal\"      # Replace with your worksheet name\n",
        "\n",
        "    # ☆\n",
        "    # Date range configuration\n",
        "    start_date = \"2012-01-01\"\n",
        "    end_date = \"2022-12-31\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = RedditInflationClassifier(model_endpoint, client)\n",
        "\n",
        "    # Load and prepare data with date filtering\n",
        "    df = load_and_prepare_data(input_file, start_date, end_date)\n",
        "\n",
        "    # ☆\n",
        "    # Sample monthly data\n",
        "    sampled_df = sample_monthly_data(df, max_samples_per_month=200)\n",
        "\n",
        "    print(f\"Using Google Spreadsheet: {spreadsheet_name}\")\n",
        "    print(f\"Using Worksheet: {worksheet_name}\")\n",
        "    if start_date:\n",
        "        print(f\"Date range: {start_date} to {end_date if end_date else 'latest'}\")\n",
        "\n",
        "    # Process and classify data with real-time Google Sheets updates\n",
        "    process_and_classify_data(sampled_df, classifier, output_file, spreadsheet_name, worksheet_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROCESS COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "    print(f\"Monthly summaries updated in Google Sheets: {spreadsheet_name} - {worksheet_name}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzYdUotsNTFW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+eXHJAtugaYqNg7K2cnM9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}