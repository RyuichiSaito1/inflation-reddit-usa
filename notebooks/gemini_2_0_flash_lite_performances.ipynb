{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNklzqYSYlqmVxqHt/a27Fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/notebooks/gemini_2_0_flash_lite_performances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAF97fbN054R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "import time\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "AXRE-Nrt09x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot"
      ],
      "metadata": {
        "id": "zB4nSf4D8ksD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "print(\"Authenticating with Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authentication completed!\")\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = \"#####\"\n",
        "LOCATION = \"#####\"\n",
        "\n",
        "print(\"Initializing Vertex AI...\")\n",
        "try:\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "    print(\"Vertex AI initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Vertex AI: {e}\")\n",
        "    print(\"Please ensure you have:\")\n",
        "    print(\"1. Set the correct PROJECT_ID\")\n",
        "    print(\"2. Enabled Vertex AI API in your project\")\n",
        "    print(\"3. Have proper permissions\")\n",
        "    raise\n",
        "\n",
        "# Initialize Gemini 2.0 Flash Lite model\n",
        "print(\"Initializing Gemini 2.0 Flash Lite model...\")\n",
        "try:\n",
        "    model = GenerativeModel(\"gemini-2.0-flash-lite\")\n",
        "    print(\"Model initialized successfully!\")\n",
        "\n",
        "    # Test the model with a simple query\n",
        "    print(\"Testing model connection...\")\n",
        "    test_response = model.generate_content(\"Hello, respond with 'OK'\")\n",
        "    print(f\"Model test response: {test_response.text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing model: {e}\")\n",
        "    print(\"Trying alternative model names...\")\n",
        "\n",
        "    # Try alternative model names\n",
        "    alternative_models = [\n",
        "        \"gemini-2.0-flash-exp\",\n",
        "        \"gemini-1.5-flash\",\n",
        "        \"gemini-1.5-pro\"\n",
        "    ]\n",
        "\n",
        "    model = None\n",
        "    for model_name in alternative_models:\n",
        "        try:\n",
        "            print(f\"Trying {model_name}...\")\n",
        "            model = GenerativeModel(model_name)\n",
        "            test_response = model.generate_content(\"Hello, respond with 'OK'\")\n",
        "            print(f\"Successfully connected to {model_name}\")\n",
        "            print(f\"Test response: {test_response.text}\")\n",
        "            break\n",
        "        except Exception as model_error:\n",
        "            print(f\"Failed to connect to {model_name}: {model_error}\")\n",
        "            continue\n",
        "\n",
        "    if model is None:\n",
        "        print(\"Could not connect to any Gemini model. Please check:\")\n",
        "        print(\"1. Your project has access to Gemini models\")\n",
        "        print(\"2. The model name is correct\")\n",
        "        print(\"3. Vertex AI API is enabled\")\n",
        "        raise Exception(\"No Gemini model available\")\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "df = pd.read_csv(test_data_path)\n",
        "\n",
        "print(f\"Test data shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")"
      ],
      "metadata": {
        "id": "VQIwQgkw3PSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classification prompt template\n",
        "def create_classification_prompt(text: str) -> str:\n",
        "    \"\"\"Create a prompt for three-class inflation perception classification.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "\n",
        "    0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "\n",
        "    2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "\n",
        "    1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "\n",
        "    Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "    Reddit post to classify: \"{text}\"\n",
        "\n",
        "    Respond with only the number: 0, 1, or 2\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "def classify_text(text: str, max_retries: int = 3) -> int:\n",
        "    \"\"\"Classify a single text using Gemini model.\"\"\"\n",
        "    prompt = create_classification_prompt(text)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Add generation config for more reliable responses\n",
        "            generation_config = {\n",
        "                \"temperature\": 0.1,\n",
        "                \"top_p\": 0.8,\n",
        "                \"top_k\": 40,\n",
        "                \"max_output_tokens\": 10,\n",
        "            }\n",
        "\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            prediction = response.text.strip()\n",
        "\n",
        "            # Clean and validate prediction - extract first digit\n",
        "            digits = re.findall(r'\\d', prediction)\n",
        "            if digits:\n",
        "                pred_num = int(digits[0])\n",
        "                if pred_num in [0, 1, 2]:\n",
        "                    return pred_num\n",
        "\n",
        "            # Try to find valid number in full response\n",
        "            for valid_class in [0, 1, 2]:\n",
        "                if str(valid_class) in prediction:\n",
        "                    return valid_class\n",
        "\n",
        "            print(f\"Invalid prediction: '{prediction}', retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in attempt {attempt + 1}: {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "    # Return 1 (neutral) as default if all attempts fail\n",
        "    print(\"All attempts failed, defaulting to class 1 (neutral)\")\n",
        "    return 1\n",
        "\n",
        "def batch_classify(texts: List[str], batch_size: int = 10) -> List[int]:\n",
        "    \"\"\"Classify texts in batches with progress tracking.\"\"\"\n",
        "    predictions = []\n",
        "    total_batches = len(texts) // batch_size + (1 if len(texts) % batch_size != 0 else 0)\n",
        "\n",
        "    print(f\"Processing {len(texts)} texts in {total_batches} batches...\")\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_predictions = []\n",
        "\n",
        "        for j, text in enumerate(batch):\n",
        "            try:\n",
        "                pred = classify_text(text)\n",
        "                batch_predictions.append(pred)\n",
        "                print(f\"Batch {i//batch_size + 1}/{total_batches}, Item {j+1}/{len(batch)}: {pred}\")\n",
        "\n",
        "                # Add delay to respect API rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing text {i+j}: {str(e)}\")\n",
        "                batch_predictions.append(1)  # Default fallback to neutral\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "        # Longer delay between batches\n",
        "        if i + batch_size < len(texts):\n",
        "            time.sleep(2)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "O7Z1Jftw39kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform classification\n",
        "print(\"\\nStarting classification...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Get predictions for all test samples\n",
        "test_texts = df['body'].tolist()\n",
        "predictions = batch_classify(test_texts, batch_size=5)  # Smaller batch size for stability\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nClassification completed in {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "k1yP1bD54Wwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for evaluation\n",
        "y_true = df['inflation'].tolist()\n",
        "y_pred = predictions\n",
        "\n",
        "# Ensure both lists have the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_len = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_len]\n",
        "    y_pred = y_pred[:min_len]\n",
        "    print(f\"Truncated to {min_len} samples for evaluation\")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Overall accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Macro-averaged metrics (average across classes)\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"\\nMacro-averaged Metrics:\")\n",
        "print(f\"Precision: {precision_macro:.4f}\")\n",
        "print(f\"Recall: {recall_macro:.4f}\")\n",
        "print(f\"F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Micro-averaged metrics (overall across all samples)\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "print(f\"\\nMicro-averaged Metrics:\")\n",
        "print(f\"Precision: {precision_micro:.4f}\")\n",
        "print(f\"Recall: {recall_micro:.4f}\")\n",
        "print(f\"F1 Score: {f1_micro:.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "classes = [0, 1, 2]  # Ensure consistent numeric ordering\n",
        "precision_per_class = precision_score(y_true, y_pred, labels=classes, average=None, zero_division=0)\n",
        "recall_per_class = recall_score(y_true, y_pred, labels=classes, average=None, zero_division=0)\n",
        "f1_per_class = f1_score(y_true, y_pred, labels=classes, average=None, zero_division=0)\n",
        "\n",
        "class_names = {0: \"Deflation\", 1: \"Neutral\", 2: \"Inflation\"}\n",
        "print(f\"\\nPer-class Metrics:\")\n",
        "for i, class_num in enumerate(classes):\n",
        "    print(f\"\\nClass {class_num} ({class_names[class_num]}):\")\n",
        "    print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
        "    print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
        "    print(f\"  F1 Score: {f1_per_class[i]:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\n\\nDetailed Classification Report:\")\n",
        "target_names = [\"Deflation (0)\", \"Neutral (1)\", \"Inflation (2)\"]\n",
        "print(classification_report(y_true, y_pred, labels=classes, target_names=target_names, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "print(cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Gemini 2.0 Flash Lite\\nInflation Perception Classification')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create results summary DataFrame\n",
        "results_summary = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1',\n",
        "               'Micro Precision', 'Micro Recall', 'Micro F1'],\n",
        "    'Score': [accuracy, precision_macro, recall_macro, f1_macro,\n",
        "              precision_micro, recall_micro, f1_micro]\n",
        "})\n",
        "\n",
        "print(f\"\\n\\nResults Summary:\")\n",
        "print(results_summary.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Save results to CSV\n",
        "results_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/gemini_evaluation_results.csv'\n",
        "results_df = pd.DataFrame({\n",
        "    'body': test_texts[:len(y_pred)],\n",
        "    'true_label': y_true,\n",
        "    'predicted_label': y_pred\n",
        "})\n",
        "\n",
        "results_df.to_csv(results_path, index=False)\n",
        "print(f\"\\nDetailed results saved to: {results_path}\")\n",
        "\n",
        "# Additional analysis: prediction distribution\n",
        "print(f\"\\n\\nPrediction Distribution:\")\n",
        "pred_counts = pd.Series(y_pred).value_counts()\n",
        "true_counts = pd.Series(y_true).value_counts()\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'True_Count': true_counts,\n",
        "    'Predicted_Count': pred_counts\n",
        "}).fillna(0)\n",
        "\n",
        "print(comparison_df)\n",
        "\n",
        "# Calculate class-wise accuracy\n",
        "print(f\"\\n\\nClass-wise Accuracy:\")\n",
        "for class_num in classes:\n",
        "    class_mask = [i for i, label in enumerate(y_true) if label == class_num]\n",
        "    if class_mask:\n",
        "        class_accuracy = sum(1 for i in class_mask if y_pred[i] == y_true[i]) / len(class_mask)\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): {class_accuracy:.4f}\")\n",
        "    else:\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): No samples in test set\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"{'='*50}\")\n"
      ],
      "metadata": {
        "id": "fi4jFyM24e6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "xoz6223J-g-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install google-genai pandas scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google import genai\n",
        "from google.genai.types import HttpOptions\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Authenticate with Google Cloud (run this first)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "bpnfxhpFRRtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1,040"
      ],
      "metadata": {
        "id": "R_1nzx7j_qQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client for Vertex AI\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "tuning_job_name = \"#####\"\n",
        "\n",
        "# Get the tuning job and the tuned model\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = \"/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv\"\n",
        "df = pd.read_csv(test_data_path)\n",
        "print(f\"Loaded {len(df)} test samples\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "h-1UOKUeRn7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "def create_prompt(reddit_post):\n",
        "    return f\"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post:\n",
        "'{reddit_post}'\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "# Function to extract classification from response\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"Extract classification number from model response\"\"\"\n",
        "    # Look for patterns like \"Classification: 2\" or just \"2\" at the end\n",
        "    patterns = [\n",
        "        r'Classification.*?([012])',\n",
        "        r'Answer.*?([012])',\n",
        "        r'Response.*?([012])',\n",
        "        r'\\b([012])\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text)\n",
        "        if matches:\n",
        "            return int(matches[-1])  # Take the last match\n",
        "\n",
        "    # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_text:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    return None  # Unable to extract classification\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model_endpoint, test_df, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîπ Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "\n",
        "    text_column = 'text' if 'text' in test_df.columns else test_df.columns[0]\n",
        "    label_column = 'label' if 'label' in test_df.columns else test_df.columns[1]\n",
        "\n",
        "    print(f\"Using text column: '{text_column}' and label column: '{label_column}'\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        reddit_post = row[text_column]\n",
        "        true_label = row[label_column]\n",
        "\n",
        "        try:\n",
        "            # Create prompt\n",
        "            prompt = create_prompt(reddit_post)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = client.models.generate_content(\n",
        "                model=model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "\n",
        "            # Extract classification\n",
        "            predicted_label = extract_classification(response.text)\n",
        "\n",
        "            if predicted_label is not None:\n",
        "                predictions.append(predicted_label)\n",
        "                true_labels.append(true_label)\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "            else:\n",
        "                failed_predictions += 1\n",
        "                print(f\"Failed to extract classification from response: {response.text[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            print(f\"Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "        # Add small delay to avoid rate limiting\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(predictions) > 0:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nüìä RESULTS for {model_name}:\")\n",
        "        print(f\"Total samples processed: {len(predictions)}/{len(test_df)}\")\n",
        "        print(f\"Failed predictions: {failed_predictions}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Deflation (0)', 'Neutral (1)', 'Inflation (2)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        print(\"True\\\\Pred    0    1    2\")\n",
        "        for i, row in enumerate(cm):\n",
        "            print(f\"    {i}    {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'failed_predictions': failed_predictions\n",
        "        }\n",
        "    else:\n",
        "        print(f\"‚ùå No valid predictions obtained for {model_name}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "C8kkfy3CRr8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the default checkpoint (tuned model endpoint)\n",
        "default_results = evaluate_model(tuning_job.tuned_model.endpoint, df, \"DEFAULT Checkpoint\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "Fux_y9SLTA4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 520"
      ],
      "metadata": {
        "id": "Tkazyn8oBkcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client for Vertex AI\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "tuning_job_name = \"#####\"\n",
        "\n",
        "# Get the tuning job and the tuned model\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = \"/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv\"\n",
        "df = pd.read_csv(test_data_path)\n",
        "print(f\"Loaded {len(df)} test samples\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "vrwPe2iCRSHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "def create_prompt(reddit_post):\n",
        "    return f\"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post:\n",
        "'{reddit_post}'\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "# Function to extract classification from response\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"Extract classification number from model response\"\"\"\n",
        "    # Look for patterns like \"Classification: 2\" or just \"2\" at the end\n",
        "    patterns = [\n",
        "        r'Classification.*?([012])',\n",
        "        r'Answer.*?([012])',\n",
        "        r'Response.*?([012])',\n",
        "        r'\\b([012])\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text)\n",
        "        if matches:\n",
        "            return int(matches[-1])  # Take the last match\n",
        "\n",
        "    # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_text:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    return None  # Unable to extract classification\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model_endpoint, test_df, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîπ Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "\n",
        "    text_column = 'text' if 'text' in test_df.columns else test_df.columns[0]\n",
        "    label_column = 'label' if 'label' in test_df.columns else test_df.columns[1]\n",
        "\n",
        "    print(f\"Using text column: '{text_column}' and label column: '{label_column}'\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        reddit_post = row[text_column]\n",
        "        true_label = row[label_column]\n",
        "\n",
        "        try:\n",
        "            # Create prompt\n",
        "            prompt = create_prompt(reddit_post)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = client.models.generate_content(\n",
        "                model=model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "\n",
        "            # Extract classification\n",
        "            predicted_label = extract_classification(response.text)\n",
        "\n",
        "            if predicted_label is not None:\n",
        "                predictions.append(predicted_label)\n",
        "                true_labels.append(true_label)\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "            else:\n",
        "                failed_predictions += 1\n",
        "                print(f\"Failed to extract classification from response: {response.text[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            print(f\"Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "        # Add small delay to avoid rate limiting\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(predictions) > 0:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nüìä RESULTS for {model_name}:\")\n",
        "        print(f\"Total samples processed: {len(predictions)}/{len(test_df)}\")\n",
        "        print(f\"Failed predictions: {failed_predictions}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Deflation (0)', 'Neutral (1)', 'Inflation (2)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        print(\"True\\\\Pred    0    1    2\")\n",
        "        for i, row in enumerate(cm):\n",
        "            print(f\"    {i}    {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'failed_predictions': failed_predictions\n",
        "        }\n",
        "    else:\n",
        "        print(f\"‚ùå No valid predictions obtained for {model_name}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "gybfGVTkBXm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the default checkpoint (tuned model endpoint)\n",
        "default_results = evaluate_model(tuning_job.tuned_model.endpoint, df, \"DEFAULT Checkpoint\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "YDeIYpngBc3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 260"
      ],
      "metadata": {
        "id": "xwBsrCN-2znC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client for Vertex AI\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "tuning_job_name = \"#####\"\n",
        "\n",
        "# Get the tuning job and the tuned model\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = \"/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv\"\n",
        "df = pd.read_csv(test_data_path)\n",
        "print(f\"Loaded {len(df)} test samples\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "1q6fYE9cBh3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "def create_prompt(reddit_post):\n",
        "    return f\"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post:\n",
        "'{reddit_post}'\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "# Function to extract classification from response\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"Extract classification number from model response\"\"\"\n",
        "    # Look for patterns like \"Classification: 2\" or just \"2\" at the end\n",
        "    patterns = [\n",
        "        r'Classification.*?([012])',\n",
        "        r'Answer.*?([012])',\n",
        "        r'Response.*?([012])',\n",
        "        r'\\b([012])\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text)\n",
        "        if matches:\n",
        "            return int(matches[-1])  # Take the last match\n",
        "\n",
        "    # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_text:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    return None  # Unable to extract classification\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model_endpoint, test_df, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîπ Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "\n",
        "    text_column = 'text' if 'text' in test_df.columns else test_df.columns[0]\n",
        "    label_column = 'label' if 'label' in test_df.columns else test_df.columns[1]\n",
        "\n",
        "    print(f\"Using text column: '{text_column}' and label column: '{label_column}'\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        reddit_post = row[text_column]\n",
        "        true_label = row[label_column]\n",
        "\n",
        "        try:\n",
        "            # Create prompt\n",
        "            prompt = create_prompt(reddit_post)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = client.models.generate_content(\n",
        "                model=model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "\n",
        "            # Extract classification\n",
        "            predicted_label = extract_classification(response.text)\n",
        "\n",
        "            if predicted_label is not None:\n",
        "                predictions.append(predicted_label)\n",
        "                true_labels.append(true_label)\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "            else:\n",
        "                failed_predictions += 1\n",
        "                print(f\"Failed to extract classification from response: {response.text[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            print(f\"Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "        # Add small delay to avoid rate limiting\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(predictions) > 0:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nüìä RESULTS for {model_name}:\")\n",
        "        print(f\"Total samples processed: {len(predictions)}/{len(test_df)}\")\n",
        "        print(f\"Failed predictions: {failed_predictions}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Deflation (0)', 'Neutral (1)', 'Inflation (2)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        print(\"True\\\\Pred    0    1    2\")\n",
        "        for i, row in enumerate(cm):\n",
        "            print(f\"    {i}    {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'failed_predictions': failed_predictions\n",
        "        }\n",
        "    else:\n",
        "        print(f\"‚ùå No valid predictions obtained for {model_name}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "dOvjVCIZ3Ed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the default checkpoint (tuned model endpoint)\n",
        "default_results = evaluate_model(tuning_job.tuned_model.endpoint, df, \"DEFAULT Checkpoint\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "UHh3_m5M3S0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 130"
      ],
      "metadata": {
        "id": "eWowwuC-3W4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client for Vertex AI\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "tuning_job_name = \"#####\"\n",
        "\n",
        "# Get the tuning job and the tuned model\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = \"/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv\"\n",
        "df = pd.read_csv(test_data_path)\n",
        "print(f\"Loaded {len(df)} test samples\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "EbMqa3Vl3WKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "def create_prompt(reddit_post):\n",
        "    return f\"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post:\n",
        "'{reddit_post}'\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "# Function to extract classification from response\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"Extract classification number from model response\"\"\"\n",
        "    # Look for patterns like \"Classification: 2\" or just \"2\" at the end\n",
        "    patterns = [\n",
        "        r'Classification.*?([012])',\n",
        "        r'Answer.*?([012])',\n",
        "        r'Response.*?([012])',\n",
        "        r'\\b([012])\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text)\n",
        "        if matches:\n",
        "            return int(matches[-1])  # Take the last match\n",
        "\n",
        "    # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_text:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    return None  # Unable to extract classification\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model_endpoint, test_df, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîπ Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "\n",
        "    text_column = 'text' if 'text' in test_df.columns else test_df.columns[0]\n",
        "    label_column = 'label' if 'label' in test_df.columns else test_df.columns[1]\n",
        "\n",
        "    print(f\"Using text column: '{text_column}' and label column: '{label_column}'\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        reddit_post = row[text_column]\n",
        "        true_label = row[label_column]\n",
        "\n",
        "        try:\n",
        "            # Create prompt\n",
        "            prompt = create_prompt(reddit_post)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = client.models.generate_content(\n",
        "                model=model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "\n",
        "            # Extract classification\n",
        "            predicted_label = extract_classification(response.text)\n",
        "\n",
        "            if predicted_label is not None:\n",
        "                predictions.append(predicted_label)\n",
        "                true_labels.append(true_label)\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "            else:\n",
        "                failed_predictions += 1\n",
        "                print(f\"Failed to extract classification from response: {response.text[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            print(f\"Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "        # Add small delay to avoid rate limiting\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(predictions) > 0:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nüìä RESULTS for {model_name}:\")\n",
        "        print(f\"Total samples processed: {len(predictions)}/{len(test_df)}\")\n",
        "        print(f\"Failed predictions: {failed_predictions}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Deflation (0)', 'Neutral (1)', 'Inflation (2)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        print(\"True\\\\Pred    0    1    2\")\n",
        "        for i, row in enumerate(cm):\n",
        "            print(f\"    {i}    {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'failed_predictions': failed_predictions\n",
        "        }\n",
        "    else:\n",
        "        print(f\"‚ùå No valid predictions obtained for {model_name}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "rXJWh31Q3hD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the default checkpoint (tuned model endpoint)\n",
        "default_results = evaluate_model(tuning_job.tuned_model.endpoint, df, \"DEFAULT Checkpoint\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "vFNIdc8H3kBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 65"
      ],
      "metadata": {
        "id": "BfveTd_Z4x7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client for Vertex AI\n",
        "project_id = \"#####\"\n",
        "location = \"#####\"\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location,\n",
        "    http_options=HttpOptions(api_version=\"v1\")\n",
        ")\n",
        "\n",
        "tuning_job_name = \"#####\"\n",
        "\n",
        "# Get the tuning job and the tuned model\n",
        "tuning_job = client.tunings.get(name=tuning_job_name)\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_data_path = \"/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv\"\n",
        "df = pd.read_csv(test_data_path)\n",
        "print(f\"Loaded {len(df)} test samples\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "NODTuKwW4N9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "def create_prompt(reddit_post):\n",
        "    return f\"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories:\n",
        "0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market.\n",
        "2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble.\n",
        "1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text.\n",
        "Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "Reddit Post:\n",
        "'{reddit_post}'\n",
        "Classification (0, 1, or 2):\"\"\"\n",
        "\n",
        "# Function to extract classification from response\n",
        "def extract_classification(response_text):\n",
        "    \"\"\"Extract classification number from model response\"\"\"\n",
        "    # Look for patterns like \"Classification: 2\" or just \"2\" at the end\n",
        "    patterns = [\n",
        "        r'Classification.*?([012])',\n",
        "        r'Answer.*?([012])',\n",
        "        r'Response.*?([012])',\n",
        "        r'\\b([012])\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text)\n",
        "        if matches:\n",
        "            return int(matches[-1])  # Take the last match\n",
        "\n",
        "    # If no clear pattern, look for the first occurrence of 0, 1, or 2\n",
        "    for char in response_text:\n",
        "        if char in ['0', '1', '2']:\n",
        "            return int(char)\n",
        "\n",
        "    return None  # Unable to extract classification\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model_endpoint, test_df, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîπ Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "\n",
        "    text_column = 'text' if 'text' in test_df.columns else test_df.columns[0]\n",
        "    label_column = 'label' if 'label' in test_df.columns else test_df.columns[1]\n",
        "\n",
        "    print(f\"Using text column: '{text_column}' and label column: '{label_column}'\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        reddit_post = row[text_column]\n",
        "        true_label = row[label_column]\n",
        "\n",
        "        try:\n",
        "            # Create prompt\n",
        "            prompt = create_prompt(reddit_post)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = client.models.generate_content(\n",
        "                model=model_endpoint,\n",
        "                contents=prompt,\n",
        "            )\n",
        "\n",
        "            # Extract classification\n",
        "            predicted_label = extract_classification(response.text)\n",
        "\n",
        "            if predicted_label is not None:\n",
        "                predictions.append(predicted_label)\n",
        "                true_labels.append(true_label)\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "            else:\n",
        "                failed_predictions += 1\n",
        "                print(f\"Failed to extract classification from response: {response.text[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            print(f\"Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "        # Add small delay to avoid rate limiting\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(predictions) > 0:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nüìä RESULTS for {model_name}:\")\n",
        "        print(f\"Total samples processed: {len(predictions)}/{len(test_df)}\")\n",
        "        print(f\"Failed predictions: {failed_predictions}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Deflation (0)', 'Neutral (1)', 'Inflation (2)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        print(\"True\\\\Pred    0    1    2\")\n",
        "        for i, row in enumerate(cm):\n",
        "            print(f\"    {i}    {row[0]:4d} {row[1]:4d} {row[2]:4d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'failed_predictions': failed_predictions\n",
        "        }\n",
        "    else:\n",
        "        print(f\"‚ùå No valid predictions obtained for {model_name}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "H7AbtFGG4TRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the default checkpoint (tuned model endpoint)\n",
        "default_results = evaluate_model(tuning_job.tuned_model.endpoint, df, \"DEFAULT Checkpoint\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "zHy0O6z44XJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dMJuWpe4ukj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}