{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMpQgImYlpX2q5b7udVLrQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/notebooks/llama_3_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using A100 GPU"
      ],
      "metadata": {
        "id": "eFZ11VIi37Yq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxR2YsQdedJd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.44.0\n",
        "!pip install datasets scikit-learn matplotlib torch torchvision torchaudio\n",
        "!pip install accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "9LGC6E9TegGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1,050"
      ],
      "metadata": {
        "id": "gCA9cX_jbIur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TimeTrackerCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        print(f\"Epoch {state.epoch} training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define the IMF economist prompt\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\""
      ],
      "metadata": {
        "id": "bt_Ypaalqn53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-1040.csv', sep=',')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")\n",
        "\n",
        "# Add formatted prompt to each post\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "df['formatted_body'] = df['body'].apply(format_with_prompt)\n",
        "\n",
        "# 2. Split into training and validation sets (75:25 ratio)\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['inflation'])\n",
        "\n",
        "# 3. Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# 4. Initialize tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better L4 GPU compatibility\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure the model uses the correct pad token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 5. Tokenization function - now using formatted_body with prompt\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_body'],  # Use formatted_body instead of body\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized['labels'] = examples['inflation']\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# 6. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "BPw-0zSu7ot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning\",\n",
        "    logging_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning/logs\",\n",
        "\n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate=5e-5,  # Slightly higher for Llama\n",
        "    num_train_epochs=4,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for 3B model\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Model selection\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,  # Fixed: accuracy should be greater_is_better=True\n",
        "\n",
        "    # Efficiency settings\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    bf16=True,  # Use BF16 instead of FP16 for L4 GPU compatibility\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Reporting\n",
        "    run_name=\"llama-3.2-3b-inflation-1040\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "FZWMrIae8J6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TimeTrackerCallback()]\n",
        ")\n",
        "\n",
        "# 9. Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "n1LSOXfZ-Sgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 540"
      ],
      "metadata": {
        "id": "vBblwUKYsa5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TimeTrackerCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        print(f\"Epoch {state.epoch} training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define the IMF economist prompt\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\""
      ],
      "metadata": {
        "id": "lYnMycE2-Ur1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-520.csv', sep=',')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")\n",
        "\n",
        "# Add formatted prompt to each post\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "df['formatted_body'] = df['body'].apply(format_with_prompt)\n",
        "\n",
        "# 2. Split into training and validation sets (75:25 ratio)\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['inflation'])\n",
        "\n",
        "# 3. Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# 4. Initialize tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better L4 GPU compatibility\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure the model uses the correct pad token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 5. Tokenization function - now using formatted_body with prompt\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_body'],  # Use formatted_body instead of body\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized['labels'] = examples['inflation']\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# 6. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "ujN3BYEhbTTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-520\",\n",
        "    logging_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-520/logs\",\n",
        "\n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate=5e-5,  # Slightly higher for Llama\n",
        "    num_train_epochs=4,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for 3B model\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Model selection\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,  # Fixed: accuracy should be greater_is_better=True\n",
        "\n",
        "    # Efficiency settings\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    bf16=True,  # Use BF16 instead of FP16 for L4 GPU compatibility\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Reporting\n",
        "    run_name=\"llama-3.2-3b-inflation-520\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "pImQjYxHb9lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TimeTrackerCallback()]\n",
        ")\n",
        "\n",
        "# 9. Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sRU7q8pCcdM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 260"
      ],
      "metadata": {
        "id": "3fGKzf4OsekP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TimeTrackerCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        print(f\"Epoch {state.epoch} training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define the IMF economist prompt\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\""
      ],
      "metadata": {
        "id": "nypJvP8vch49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-260.csv', sep=',')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")\n",
        "\n",
        "# Add formatted prompt to each post\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "df['formatted_body'] = df['body'].apply(format_with_prompt)\n",
        "\n",
        "# 2. Split into training and validation sets (75:25 ratio)\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['inflation'])\n",
        "\n",
        "# 3. Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# 4. Initialize tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better L4 GPU compatibility\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure the model uses the correct pad token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 5. Tokenization function - now using formatted_body with prompt\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_body'],  # Use formatted_body instead of body\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized['labels'] = examples['inflation']\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# 6. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "_E9lF_iNslTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-260\",\n",
        "    logging_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-260/logs\",\n",
        "\n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate=5e-5,  # Slightly higher for Llama\n",
        "    num_train_epochs=4,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for 3B model\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Model selection\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,  # Fixed: accuracy should be greater_is_better=True\n",
        "\n",
        "    # Efficiency settings\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    bf16=True,  # Use BF16 instead of FP16 for L4 GPU compatibility\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Reporting\n",
        "    run_name=\"llama-3.2-3b-inflation-520\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "pLw0CqpRsppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TimeTrackerCallback()]\n",
        ")\n",
        "\n",
        "# 9. Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_Zq74dDWsrvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 130"
      ],
      "metadata": {
        "id": "lPa05iCj0oK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TimeTrackerCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        print(f\"Epoch {state.epoch} training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define the IMF economist prompt\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\""
      ],
      "metadata": {
        "id": "Ruy_FM2Ns0RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-130.csv', sep=',')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")\n",
        "\n",
        "# Add formatted prompt to each post\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "df['formatted_body'] = df['body'].apply(format_with_prompt)\n",
        "\n",
        "# 2. Split into training and validation sets (75:25 ratio)\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['inflation'])\n",
        "\n",
        "# 3. Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# 4. Initialize tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better L4 GPU compatibility\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure the model uses the correct pad token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 5. Tokenization function - now using formatted_body with prompt\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_body'],  # Use formatted_body instead of body\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized['labels'] = examples['inflation']\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# 6. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "IQTtEahd00vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-130\",\n",
        "    logging_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-130/logs\",\n",
        "\n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate=5e-5,  # Slightly higher for Llama\n",
        "    num_train_epochs=4,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for 3B model\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Model selection\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,  # Fixed: accuracy should be greater_is_better=True\n",
        "\n",
        "    # Efficiency settings\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    bf16=True,  # Use BF16 instead of FP16 for L4 GPU compatibility\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Reporting\n",
        "    run_name=\"llama-3.2-3b-inflation-520\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "IFidFC-C034c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TimeTrackerCallback()]\n",
        ")\n",
        "\n",
        "# 9. Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "57Bye-AU06ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 65"
      ],
      "metadata": {
        "id": "GNCpY-qm61ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TimeTrackerCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        print(f\"Epoch {state.epoch} training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define the IMF economist prompt\n",
        "INFLATION_PROMPT = \"\"\"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., \"the prices are not bad\"), affordable services (e.g., \"this champagne is cheap and delicious\"), sales information (e.g., \"you can get it for only 10 dollars.\"), or a declining and buyer's market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., \"it's not cheap\"), the unreasonable cost of goods or services (e.g., \"the food is overpriced and cold\"), consumers struggling to afford necessities (e.g., \"items are too expensive to buy\"), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., \"a gorgeous and costly dinner\" or \"an affordable Civic\"), website promotion, authors' wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\n",
        "\n",
        "Reddit Post: {post}\n",
        "\n",
        "Classification:\"\"\""
      ],
      "metadata": {
        "id": "AK18J1YH60vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/world-inflation/data/reddit/production/main-prod-65.csv', sep=',')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['inflation'].value_counts()}\")\n",
        "\n",
        "# Add formatted prompt to each post\n",
        "def format_with_prompt(post):\n",
        "    return INFLATION_PROMPT.format(post=post)\n",
        "\n",
        "df['formatted_body'] = df['body'].apply(format_with_prompt)\n",
        "\n",
        "# 2. Split into training and validation sets (75:25 ratio)\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['inflation'])\n",
        "\n",
        "# 3. Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# 4. Initialize tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better L4 GPU compatibility\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure the model uses the correct pad token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 5. Tokenization function - now using formatted_body with prompt\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_body'],  # Use formatted_body instead of body\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized['labels'] = examples['inflation']\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# 6. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "urEi9da41JvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-65\",\n",
        "    logging_dir=\"/content/drive/MyDrive/world-inflation/data/model/llama-3.2-3b-fine-tuning-65/logs\",\n",
        "\n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate=5e-5,  # Slightly higher for Llama\n",
        "    num_train_epochs=4,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for 3B model\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Model selection\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,  # Fixed: accuracy should be greater_is_better=True\n",
        "\n",
        "    # Efficiency settings\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    bf16=True,  # Use BF16 instead of FP16 for L4 GPU compatibility\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Reporting\n",
        "    run_name=\"llama-3.2-3b-inflation-65\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "hpN7sB4B6-lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TimeTrackerCallback()]\n",
        ")\n",
        "\n",
        "# 9. Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_Yj7b3AU7F-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBYGUX-P7K6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}