{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVuaWXdNCTIcURUX1GJ3G1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/notebooks/gpt_4_1_mini_performances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-W6uhT9d17c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# https://community.openai.com/t/google-colab-fine-tuning-error/5917\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"#####\""
      ],
      "metadata": {
        "id": "xWhIU4Mkd-Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training + validation: 1243"
      ],
      "metadata": {
        "id": "yPAZhgx5uQmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def read_csv_data(file_path):\n",
        "    data = pd.read_csv(file_path, usecols=['body', 'inflation'], dtype='object', engine='python')\n",
        "    data = data.rename(columns={'body': 'content', 'inflation': 'label'})\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-1243.csv'\n",
        "\n",
        "test_data_raw = read_csv_data(file_path)\n",
        "\n",
        "print(f\"Input data count: {len(test_data_raw)}\")\n",
        "\n",
        "test_data_deduplicated = test_data_raw.drop_duplicates(subset=['content', 'label'])\n",
        "\n",
        "print(f\"Deduplicated data count: {len(test_data_deduplicated)}\")\n",
        "\n",
        "test_data_shuffled = test_data_deduplicated.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Shuffled data count: {len(test_data_shuffled)}\")\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-4.1-mini-2025-04-14:university-of-tsukuba:2025-05-16:BfRHWjKq\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., “the prices are not bad”), affordable services (e.g., “this champagne is cheap and delicious”), sales information (e.g., “you can get it for only 10 dollars.”), or a declining and buyer’s  market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., “it’s not cheap”), the unreasonable cost of goods or services (e.g., “the food is overpriced and cold”), consumers struggling to afford necessities (e.g., “items are too expensive to buy”), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., “a gorgeous and costly dinner” or “an affordable Civic”), website promotion, authors’ wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Text being evaluated: \" + text)\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for _, example in test_data_shuffled.iterrows():\n",
        "    text = example['content']\n",
        "\n",
        "    print(f\"\\n--- New Evaluation ---\")\n",
        "    print(f\"Text: {text}\")\n",
        "\n",
        "    true_label = int(example['label'])\n",
        "\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "if true_labels and predicted_labels:\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    # Added zero_division=0 to handle cases where a class might not be predicted\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    macro_avg_precision = precision.mean()\n",
        "    macro_avg_recall = recall.mean()\n",
        "    macro_avg_f1 = f1.mean()\n",
        "\n",
        "    # Micro average for precision, recall, F1 is equivalent to accuracy in multiclass\n",
        "    micro_avg_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(len(recall)):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {macro_avg_recall:.2f}   |   {macro_avg_precision:.2f}   |   {macro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {micro_avg_recall:.2f}   |   {micro_avg_precision:.2f}   |   {micro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "else:\n",
        "    print(\"\\nNo predictions were made, or no valid labels were found. Skipping evaluation metrics.\")"
      ],
      "metadata": {
        "id": "n1wOlhLlz2WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def read_csv_data(file_path):\n",
        "    data = pd.read_csv(file_path, usecols=['body', 'inflation'], dtype='object', engine='python')\n",
        "    data = data.rename(columns={'body': 'content', 'inflation': 'label'})\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "test_data_raw = read_csv_data(file_path)\n",
        "\n",
        "print(f\"Input data count: {len(test_data_raw)}\")\n",
        "\n",
        "test_data_deduplicated = test_data_raw.drop_duplicates(subset=['content', 'label'])\n",
        "\n",
        "print(f\"Deduplicated data count: {len(test_data_deduplicated)}\")\n",
        "\n",
        "test_data_shuffled = test_data_deduplicated.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Shuffled data count: {len(test_data_shuffled)}\")\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-4.1-mini-2025-04-14:university-of-tsukuba:2025-05-16:BfRHWjKq\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., “the prices are not bad”), affordable services (e.g., “this champagne is cheap and delicious”), sales information (e.g., “you can get it for only 10 dollars.”), or a declining and buyer’s  market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., “it’s not cheap”), the unreasonable cost of goods or services (e.g., “the food is overpriced and cold”), consumers struggling to afford necessities (e.g., “items are too expensive to buy”), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., “a gorgeous and costly dinner” or “an affordable Civic”), website promotion, authors’ wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Text being evaluated: \" + text)\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for _, example in test_data_shuffled.iterrows():\n",
        "    text = example['content']\n",
        "\n",
        "    print(f\"\\n--- New Evaluation ---\")\n",
        "    print(f\"Text: {text}\")\n",
        "\n",
        "    true_label = int(example['label'])\n",
        "\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "if true_labels and predicted_labels:\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    # Added zero_division=0 to handle cases where a class might not be predicted\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    macro_avg_precision = precision.mean()\n",
        "    macro_avg_recall = recall.mean()\n",
        "    macro_avg_f1 = f1.mean()\n",
        "\n",
        "    # Micro average for precision, recall, F1 is equivalent to accuracy in multiclass\n",
        "    micro_avg_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(len(recall)):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {macro_avg_recall:.2f}   |   {macro_avg_precision:.2f}   |   {macro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {micro_avg_recall:.2f}   |   {micro_avg_precision:.2f}   |   {micro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "else:\n",
        "    print(\"\\nNo predictions were made, or no valid labels were found. Skipping evaluation metrics.\")"
      ],
      "metadata": {
        "id": "BRHumVMHXbGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training + validation: 65"
      ],
      "metadata": {
        "id": "iVyHqQlS_eqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def read_csv_data(file_path):\n",
        "    data = pd.read_csv(file_path, usecols=['body', 'inflation'], dtype='object', engine='python')\n",
        "    data = data.rename(columns={'body': 'content', 'inflation': 'label'})\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "test_data_raw = read_csv_data(file_path)\n",
        "\n",
        "print(f\"Input data count: {len(test_data_raw)}\")\n",
        "\n",
        "test_data_deduplicated = test_data_raw.drop_duplicates(subset=['content', 'label'])\n",
        "\n",
        "print(f\"Deduplicated data count: {len(test_data_deduplicated)}\")\n",
        "\n",
        "test_data_shuffled = test_data_deduplicated.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Shuffled data count: {len(test_data_shuffled)}\")\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-4.1-mini-2025-04-14:university-of-tsukuba:2025-06-07:Bfpj9sSP\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., “the prices are not bad”), affordable services (e.g., “this champagne is cheap and delicious”), sales information (e.g., “you can get it for only 10 dollars.”), or a declining and buyer’s  market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., “it’s not cheap”), the unreasonable cost of goods or services (e.g., “the food is overpriced and cold”), consumers struggling to afford necessities (e.g., “items are too expensive to buy”), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., “a gorgeous and costly dinner” or “an affordable Civic”), website promotion, authors’ wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Text being evaluated: \" + text)\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for _, example in test_data_shuffled.iterrows():\n",
        "    text = example['content']\n",
        "\n",
        "    print(f\"\\n--- New Evaluation ---\")\n",
        "    print(f\"Text: {text}\")\n",
        "\n",
        "    true_label = int(example['label'])\n",
        "\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "if true_labels and predicted_labels:\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    # Added zero_division=0 to handle cases where a class might not be predicted\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    macro_avg_precision = precision.mean()\n",
        "    macro_avg_recall = recall.mean()\n",
        "    macro_avg_f1 = f1.mean()\n",
        "\n",
        "    # Micro average for precision, recall, F1 is equivalent to accuracy in multiclass\n",
        "    micro_avg_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(len(recall)):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {macro_avg_recall:.2f}   |   {macro_avg_precision:.2f}   |   {macro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {micro_avg_recall:.2f}   |   {micro_avg_precision:.2f}   |   {micro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "else:\n",
        "    print(\"\\nNo predictions were made, or no valid labels were found. Skipping evaluation metrics.\")"
      ],
      "metadata": {
        "id": "2JjX9FaJBV6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot"
      ],
      "metadata": {
        "id": "SYpuXpJwv5Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def read_csv_data(file_path):\n",
        "    data = pd.read_csv(file_path, usecols=['body', 'inflation'], dtype='object', engine='python')\n",
        "    data = data.rename(columns={'body': 'content', 'inflation': 'label'})\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/world-inflation/data/reddit/production/test-data-200.csv'\n",
        "\n",
        "test_data_raw = read_csv_data(file_path) # Renamed variable for clarity\n",
        "\n",
        "print(f\"Input data count: {len(test_data_raw)}\")\n",
        "\n",
        "test_data_deduplicated = test_data_raw.drop_duplicates(subset=['content', 'label'])\n",
        "\n",
        "print(f\"Deduplicated data count: {len(test_data_deduplicated)}\")\n",
        "\n",
        "test_data_shuffled = test_data_deduplicated.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Shuffled data count: {len(test_data_shuffled)}\") # Added a print statement to confirm\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-mini-2025-04-14\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a chief economist at the IMF. I would like you to infer the public perception of inflation from Reddit posts. Please classify each Reddit post into one of the following categories: 0: The post indicates deflation, such as the lower price of goods or services (e.g., “the prices are not bad”), affordable services (e.g., “this champagne is cheap and delicious”), sales information (e.g., “you can get it for only 10 dollars.”), or a declining and buyer’s  market. 2: The post indicates or includes inflation, such as the higher price of goods or services (e.g., “it’s not cheap”), the unreasonable cost of goods or services (e.g., “the food is overpriced and cold”), consumers struggling to afford necessities (e.g., “items are too expensive to buy”), shortage of goods of services, or mention about an asset bubble. 1: The post indicates neither deflation (0) nor inflation (2). This category also includes just questions to a community, social statements not personal experience, factual observations, references to originally expensive or cheap goods or services (e.g., “a gorgeous and costly dinner” or “an affordable Civic”), website promotion, authors’ wishes, or illogical text. Please choose a stronger stance when the text includes both 0 and 2 stances. If these stances are of the same degree, answer 1.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Print the text being evaluated\n",
        "    print(\"Text being evaluated: \" + text)\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for _, example in test_data_shuffled.iterrows():\n",
        "    text = example['content']\n",
        "\n",
        "    print(f\"\\n--- New Evaluation ---\")\n",
        "    print(f\"Text: {text}\")\n",
        "\n",
        "    true_label = int(example['label'])\n",
        "\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "if true_labels and predicted_labels:\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    # Added zero_division=0 to handle cases where a class might not be predicted\n",
        "    recall = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    precision = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    macro_avg_precision = precision.mean()\n",
        "    macro_avg_recall = recall.mean()\n",
        "    macro_avg_f1 = f1.mean()\n",
        "\n",
        "    # Micro average for precision, recall, F1 is equivalent to accuracy in multiclass\n",
        "    micro_avg_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "    micro_avg_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "    print(\"\\n+--------------+-----------+----------+----------+----------+\")\n",
        "    print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    for i in range(len(recall)):\n",
        "        print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Macro Average|    {accuracy:.2f}   |   {macro_avg_recall:.2f}   |   {macro_avg_precision:.2f}   |   {macro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "    print(f\"| Micro Average|    {accuracy:.2f}   |   {micro_avg_recall:.2f}   |   {micro_avg_precision:.2f}   |   {micro_avg_f1:.2f}   |\")\n",
        "    print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "else:\n",
        "    print(\"\\nNo predictions were made, or no valid labels were found. Skipping evaluation metrics.\")"
      ],
      "metadata": {
        "id": "ZoSrG_xGUqpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPtmbCNVv1H-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}