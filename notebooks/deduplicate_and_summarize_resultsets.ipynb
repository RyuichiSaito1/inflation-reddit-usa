{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzUvQKzY3looVlrKoV3yav",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/inflation-reddit-usa/blob/main/notebooks/deduplicate_and_summarize_resultsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzdE9FQ7qrTc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def read_tsv_files(comments_path, submissions_path):\n",
        "\n",
        "    try:\n",
        "        comments_df = pd.read_csv(comments_path, sep='\\t', encoding='utf-8')\n",
        "        submissions_df = pd.read_csv(submissions_path, sep='\\t', encoding='utf-8')\n",
        "\n",
        "        print(f\"Comments dataset shape: {comments_df.shape}\")\n",
        "        print(f\"Submissions dataset shape: {submissions_df.shape}\")\n",
        "\n",
        "        return comments_df, submissions_df\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File not found - {e}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading files: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "u5VEucXLqtbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deduplicate_by_body(df, dataset_name):\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Remove duplicates based on 'body' column, keeping first occurrence\n",
        "    df_dedup = df.drop_duplicates(subset=['body'], keep='first')\n",
        "\n",
        "    final_count = len(df_dedup)\n",
        "    removed_count = initial_count - final_count\n",
        "\n",
        "    print(f\"{dataset_name} - Initial records: {initial_count}\")\n",
        "    print(f\"{dataset_name} - After deduplication: {final_count}\")\n",
        "    print(f\"{dataset_name} - Removed duplicates: {removed_count}\")\n",
        "\n",
        "    return df_dedup"
      ],
      "metadata": {
        "id": "CL_zGb2dq8Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tsv_file(df, output_path, filename):\n",
        "\n",
        "    try:\n",
        "        # Create directory if it doesn't exist\n",
        "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        full_path = os.path.join(output_path, filename)\n",
        "        df.to_csv(full_path, sep='\\t', index=False, encoding='utf-8')\n",
        "\n",
        "        print(f\"Successfully saved: {full_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file {filename}: {e}\")"
      ],
      "metadata": {
        "id": "TJi9rkx6rDUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_analyze_users(comments_df, submissions_df):\n",
        "\n",
        "    # Merge the datasets\n",
        "    merged_df = pd.concat([comments_df, submissions_df], ignore_index=True)\n",
        "\n",
        "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
        "\n",
        "    # Count unique users\n",
        "    unique_users = merged_df['author'].nunique()\n",
        "    total_records = len(merged_df)\n",
        "\n",
        "    # Additional user statistics\n",
        "    user_stats = {\n",
        "        'total_records': total_records,\n",
        "        'unique_users': unique_users,\n",
        "        'avg_posts_per_user': total_records / unique_users if unique_users > 0 else 0\n",
        "    }\n",
        "\n",
        "    # Top contributors analysis\n",
        "    user_counts = merged_df['author'].value_counts()\n",
        "    top_contributors = user_counts.head(10)\n",
        "\n",
        "    print(f\"\\nUser Analysis:\")\n",
        "    print(f\"Total records in merged dataset: {total_records}\")\n",
        "    print(f\"Unique users: {unique_users}\")\n",
        "    print(f\"Average posts per user: {user_stats['avg_posts_per_user']:.2f}\")\n",
        "\n",
        "    print(f\"\\nTop 10 contributors:\")\n",
        "    for author, count in top_contributors.items():\n",
        "        print(f\"  {author}: {count} posts\")\n",
        "\n",
        "    return merged_df, unique_users, user_stats"
      ],
      "metadata": {
        "id": "0997RjgfrIcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # ☆\n",
        "    # File paths\n",
        "    comments_path = '/content/drive/MyDrive/world-inflation/result/tsv/food_comments_results_202508.tsv'\n",
        "    submissions_path = '/content/drive/MyDrive/world-inflation/result/tsv/food_submissions_results.tsv'\n",
        "\n",
        "    # Specify output directory (modify as needed)\n",
        "    output_directory = '/content/drive/MyDrive/world-inflation/result/tsv/'\n",
        "\n",
        "    print(\"Starting inflation sentiment analysis data processing...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Read TSV files\n",
        "    print(\"\\n1. Reading TSV files...\")\n",
        "    comments_df, submissions_df = read_tsv_files(comments_path, submissions_path)\n",
        "\n",
        "    if comments_df is None or submissions_df is None:\n",
        "        print(\"Failed to read input files. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Display basic info about the datasets\n",
        "    print(f\"\\nDataset columns: {list(comments_df.columns)}\")\n",
        "\n",
        "    # Step 2: Deduplicate datasets\n",
        "    print(\"\\n2. Deduplicating datasets...\")\n",
        "    comments_dedup = deduplicate_by_body(comments_df, \"Comments\")\n",
        "    submissions_dedup = deduplicate_by_body(submissions_df, \"Submissions\")\n",
        "\n",
        "    # ☆\n",
        "    # Step 3: Save deduplicated datasets\n",
        "    print(\"\\n3. Saving deduplicated datasets...\")\n",
        "    save_tsv_file(comments_dedup, output_directory, 'food_comments_final_results_20260819.tsv')\n",
        "    save_tsv_file(submissions_dedup, output_directory, 'food_submissions_final_results_20260819.tsv')\n",
        "\n",
        "    # Step 4: Merge and analyze users\n",
        "    print(\"\\n4. Merging datasets and analyzing users...\")\n",
        "    merged_df, unique_users, user_stats = merge_and_analyze_users(comments_dedup, submissions_dedup)\n",
        "\n",
        "    # ☆\n",
        "    # Save merged dataset\n",
        "    save_tsv_file(merged_df, output_directory, 'merged_food_results_20260819.tsv')\n",
        "\n",
        "    # ☆\n",
        "    # Save user statistics\n",
        "    # stats_df = pd.DataFrame([user_stats])\n",
        "    # save_tsv_file(stats_df, output_directory, 'Frugal_users.tsv')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Data processing completed successfully!\")\n",
        "    print(f\"Output directory: {output_directory}\")\n",
        "    print(f\"Unique users identified: {unique_users}\")"
      ],
      "metadata": {
        "id": "PZ34bS2irNre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the main processing pipeline\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRiC9_0nsAIR",
        "outputId": "61d9192d-c1cd-4df5-aa5c-6523b1370fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting inflation sentiment analysis data processing...\n",
            "============================================================\n",
            "\n",
            "1. Reading TSV files...\n",
            "Comments dataset shape: (95731, 8)\n",
            "Submissions dataset shape: (1639, 8)\n",
            "\n",
            "Dataset columns: ['created_date', 'subreddit_id', 'id', 'author', 'parent_id', 'body', 'score', 'inflation']\n",
            "\n",
            "2. Deduplicating datasets...\n",
            "Comments - Initial records: 95731\n",
            "Comments - After deduplication: 95729\n",
            "Comments - Removed duplicates: 2\n",
            "Submissions - Initial records: 1639\n",
            "Submissions - After deduplication: 1639\n",
            "Submissions - Removed duplicates: 0\n",
            "\n",
            "3. Saving deduplicated datasets...\n",
            "Successfully saved: /content/drive/MyDrive/world-inflation/result/tsv/food_comments_final_results_20260819.tsv\n",
            "Successfully saved: /content/drive/MyDrive/world-inflation/result/tsv/food_submissions_final_results_20260819.tsv\n",
            "\n",
            "4. Merging datasets and analyzing users...\n",
            "Merged dataset shape: (97368, 8)\n",
            "\n",
            "User Analysis:\n",
            "Total records in merged dataset: 97368\n",
            "Unique users: 60569\n",
            "Average posts per user: 1.61\n",
            "\n",
            "Top 10 contributors:\n",
            "  FoodMod: 3396 posts\n",
            "  Sun_Beams: 2443 posts\n",
            "  [deleted]: 2277 posts\n",
            "  pussgurka: 931 posts\n",
            "  alcoholic_dinosaur: 425 posts\n",
            "  squid50s: 315 posts\n",
            "  MarioDesigns: 234 posts\n",
            "  TheLadyEve: 99 posts\n",
            "  Zombies_Are_Dead: 66 posts\n",
            "  randoh12: 61 posts\n",
            "Successfully saved: /content/drive/MyDrive/world-inflation/result/tsv/merged_food_results_20260819.tsv\n",
            "\n",
            "============================================================\n",
            "Data processing completed successfully!\n",
            "Output directory: /content/drive/MyDrive/world-inflation/result/tsv/\n",
            "Unique users identified: 60569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GcNCDsbsZk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}